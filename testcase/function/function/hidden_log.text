/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
[2025-12-28 09:54:17] WARNING model_config.py:1013: Casting torch.bfloat16 to torch.float16.
[2025-12-28 09:54:17] WARNING server_args.py:1972: Overlap scheduler is disabled because of using eagle3 or standalone speculative decoding.You can set env SGLANG_ENABLE_SPEC_V2=True to enable the experimental overlap scheduler.
Traceback (most recent call last):
  File "<frozen runpy>", line 198, in _run_module_as_main
  File "<frozen runpy>", line 88, in _run_code
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/launch_server.py", line 29, in <module>
    server_args = prepare_server_args(sys.argv[1:])
                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4944, in prepare_server_args
    return ServerArgs.from_cli_args(raw_args)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 4441, in from_cli_args
    return cls(**{attr: getattr(args, attr) for attr in attrs})
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "<string>", line 314, in __init__
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 702, in __post_init__
    self._handle_speculative_decoding()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/server_args.py", line 2041, in _handle_speculative_decoding
    raise ValueError(
ValueError: speculative_eagle_topk > 1 with page_size > 1 is unstable and produces incorrect results for paged attention backends. This combination is only supported for the 'flashinfer' backend.
[ERROR] 2025-12-28-09:54:18 (PID:48122, Device:-1, RankID:-1) ERR99999 UNKNOWN applicaiton exception
Eusage: launch_server.py [-h] --model-path MODEL_PATH
                        [--tokenizer-path TOKENIZER_PATH]
                        [--tokenizer-mode {auto,slow}]
                        [--tokenizer-worker-num TOKENIZER_WORKER_NUM]
                        [--skip-tokenizer-init]
                        [--load-format {auto,pt,safetensors,npcache,dummy,sharded_state,gguf,bitsandbytes,layered,flash_rl,remote,remote_instance,private}]
                        [--model-loader-extra-config MODEL_LOADER_EXTRA_CONFIG]
                        [--trust-remote-code]
                        [--context-length CONTEXT_LENGTH] [--is-embedding]
                        [--enable-multimodal]
                        [--limit-mm-data-per-request LIMIT_MM_DATA_PER_REQUEST]
                        [--revision REVISION] [--model-impl MODEL_IMPL]
                        [--host HOST] [--port PORT]
                        [--fastapi-root-path FASTAPI_ROOT_PATH] [--grpc-mode]
                        [--skip-server-warmup] [--warmups WARMUPS]
                        [--nccl-port NCCL_PORT]
                        [--checkpoint-engine-wait-weights-before-ready]
                        [--dtype {auto,half,float16,bfloat16,float,float32}]
                        [--quantization {awq,fp8,gptq,marlin,gptq_marlin,awq_marlin,bitsandbytes,gguf,modelopt,modelopt_fp8,modelopt_fp4,petit_nvfp4,w8a8_int8,w8a8_fp8,moe_wna16,qoq,w4afp8,mxfp4,auto-round,compressed-tensors,modelslim}]
                        [--quantization-param-path QUANTIZATION_PARAM_PATH]
                        [--kv-cache-dtype {auto,fp8_e5m2,fp8_e4m3,bf16,bfloat16,fp4_e2m1}]
                        [--enable-fp32-lm-head]
                        [--modelopt-quant MODELOPT_QUANT]
                        [--modelopt-checkpoint-restore-path MODELOPT_CHECKPOINT_RESTORE_PATH]
                        [--modelopt-checkpoint-save-path MODELOPT_CHECKPOINT_SAVE_PATH]
                        [--modelopt-export-path MODELOPT_EXPORT_PATH]
                        [--quantize-and-serve]
                        [--rl-quant-profile RL_QUANT_PROFILE]
                        [--mem-fraction-static MEM_FRACTION_STATIC]
                        [--max-running-requests MAX_RUNNING_REQUESTS]
                        [--max-queued-requests MAX_QUEUED_REQUESTS]
                        [--max-total-tokens MAX_TOTAL_TOKENS]
                        [--chunked-prefill-size CHUNKED_PREFILL_SIZE]
                        [--prefill-max-requests PREFILL_MAX_REQUESTS]
                        [--enable-dynamic-chunking]
                        [--max-prefill-tokens MAX_PREFILL_TOKENS]
                        [--schedule-policy {lpm,random,fcfs,dfs-weight,lof,priority}]
                        [--enable-priority-scheduling]
                        [--abort-on-priority-when-disabled]
                        [--schedule-low-priority-values-first]
                        [--priority-scheduling-preemption-threshold PRIORITY_SCHEDULING_PREEMPTION_THRESHOLD]
                        [--schedule-conservativeness SCHEDULE_CONSERVATIVENESS]
                        [--page-size PAGE_SIZE]
                        [--hybrid-kvcache-ratio [HYBRID_KVCACHE_RATIO]]
                        [--swa-full-tokens-ratio SWA_FULL_TOKENS_RATIO]
                        [--disable-hybrid-swa-memory]
                        [--radix-eviction-policy {lru,lfu}] [--device DEVICE]
                        [--tensor-parallel-size TENSOR_PARALLEL_SIZE]
                        [--pipeline-parallel-size PIPELINE_PARALLEL_SIZE]
                        [--pp-max-micro-batch-size PP_MAX_MICRO_BATCH_SIZE]
                        [--pp-async-batch-depth PP_ASYNC_BATCH_DEPTH]
                        [--stream-interval STREAM_INTERVAL] [--stream-output]
                        [--random-seed RANDOM_SEED]
                        [--constrained-json-whitespace-pattern CONSTRAINED_JSON_WHITESPACE_PATTERN]
                        [--constrained-json-disable-any-whitespace]
                        [--watchdog-timeout WATCHDOG_TIMEOUT]
                        [--soft-watchdog-timeout SOFT_WATCHDOG_TIMEOUT]
                        [--dist-timeout DIST_TIMEOUT]
                        [--download-dir DOWNLOAD_DIR]
                        [--base-gpu-id BASE_GPU_ID]
                        [--gpu-id-step GPU_ID_STEP] [--sleep-on-idle]
                        [--custom-sigquit-handler CUSTOM_SIGQUIT_HANDLER]
                        [--log-level LOG_LEVEL]
                        [--log-level-http LOG_LEVEL_HTTP] [--log-requests]
                        [--log-requests-level {0,1,2,3}]
                        [--crash-dump-folder CRASH_DUMP_FOLDER]
                        [--show-time-cost] [--enable-metrics]
                        [--enable-metrics-for-all-schedulers]
                        [--tokenizer-metrics-custom-labels-header TOKENIZER_METRICS_CUSTOM_LABELS_HEADER]
                        [--tokenizer-metrics-allowed-custom-labels TOKENIZER_METRICS_ALLOWED_CUSTOM_LABELS [TOKENIZER_METRICS_ALLOWED_CUSTOM_LABELS ...]]
                        [--bucket-time-to-first-token BUCKET_TIME_TO_FIRST_TOKEN [BUCKET_TIME_TO_FIRST_TOKEN ...]]
                        [--bucket-inter-token-latency BUCKET_INTER_TOKEN_LATENCY [BUCKET_INTER_TOKEN_LATENCY ...]]
                        [--bucket-e2e-request-latency BUCKET_E2E_REQUEST_LATENCY [BUCKET_E2E_REQUEST_LATENCY ...]]
                        [--collect-tokens-histogram]
                        [--prompt-tokens-buckets PROMPT_TOKENS_BUCKETS [PROMPT_TOKENS_BUCKETS ...]]
                        [--generation-tokens-buckets GENERATION_TOKENS_BUCKETS [GENERATION_TOKENS_BUCKETS ...]]
                        [--gc-warning-threshold-secs GC_WARNING_THRESHOLD_SECS]
                        [--decode-log-interval DECODE_LOG_INTERVAL]
                        [--enable-request-time-stats-logging]
                        [--kv-events-config KV_EVENTS_CONFIG] [--enable-trace]
                        [--otlp-traces-endpoint OTLP_TRACES_ENDPOINT]
                        [--export-metrics-to-file]
                        [--export-metrics-to-file-dir EXPORT_METRICS_TO_FILE_DIR]
                        [--api-key API_KEY]
                        [--served-model-name SERVED_MODEL_NAME]
                        [--weight-version WEIGHT_VERSION]
                        [--chat-template CHAT_TEMPLATE]
                        [--completion-template COMPLETION_TEMPLATE]
                        [--file-storage-path FILE_STORAGE_PATH]
                        [--enable-cache-report]
                        [--reasoning-parser {deepseek-r1,deepseek-v3,glm45,gpt-oss,kimi,kimi_k2,qwen3,qwen3-thinking,minimax,minimax-append-think,step3,nano_v3,interns1}]
                        [--tool-call-parser {deepseekv3,deepseekv31,deepseekv32,glm,glm45,glm47,gpt-oss,kimi_k2,llama3,mimo,mistral,pythonic,qwen,qwen25,qwen3_coder,step3,minimax-m2,interns1}]
                        [--tool-server TOOL_SERVER]
                        [--sampling-defaults {openai,model}]
                        [--data-parallel-size DATA_PARALLEL_SIZE]
                        [--load-balance-method {round_robin,decode_round_robin,shortest_queue,minimum_tokens}]
                        [--load-watch-interval LOAD_WATCH_INTERVAL]
                        [--prefill-round-robin-balance]
                        [--dist-init-addr DIST_INIT_ADDR] [--nnodes NNODES]
                        [--node-rank NODE_RANK]
                        [--json-model-override-args JSON_MODEL_OVERRIDE_ARGS]
                        [--preferred-sampling-params PREFERRED_SAMPLING_PARAMS]
                        [--enable-lora] [--max-lora-rank MAX_LORA_RANK]
                        [--lora-target-modules [{q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj,qkv_proj,gate_up_proj,embed_tokens,lm_head,all} ...]]
                        [--lora-paths [LORA_PATHS ...]]
                        [--max-loras-per-batch MAX_LORAS_PER_BATCH]
                        [--max-loaded-loras MAX_LOADED_LORAS]
                        [--lora-eviction-policy {lru,fifo}]
                        [--lora-backend {triton,csgmv,ascend,torch_native}]
                        [--max-lora-chunk-size {16,32,64,128}]
                        [--attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend,intel_xpu}]
                        [--prefill-attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend,intel_xpu}]
                        [--decode-attention-backend {triton,torch_native,flex_attention,nsa,cutlass_mla,fa3,fa4,flashinfer,flashmla,trtllm_mla,trtllm_mha,dual_chunk_flash_attn,aiter,wave,intel_amx,ascend,intel_xpu}]
                        [--sampling-backend {flashinfer,pytorch,ascend}]
                        [--grammar-backend {xgrammar,outlines,llguidance,none}]
                        [--mm-attention-backend {sdpa,fa3,triton_attn,ascend_attn,aiter_attn}]
                        [--nsa-prefill-backend {flashmla_sparse,flashmla_kv,flashmla_auto,fa3,tilelang,aiter}]
                        [--nsa-decode-backend {flashmla_sparse,flashmla_kv,flashmla_auto,fa3,tilelang,aiter}]
                        [--fp8-gemm-backend {auto,deep_gemm,flashinfer_trtllm,cutlass,triton,aiter}]
                        [--disable-flashinfer-autotune]
                        [--speculative-algorithm {EAGLE,EAGLE3,NEXTN,STANDALONE,NGRAM}]
                        [--speculative-draft-model-path SPECULATIVE_DRAFT_MODEL_PATH]
                        [--speculative-draft-model-revision SPECULATIVE_DRAFT_MODEL_REVISION]
                        [--speculative-draft-load-format {auto,pt,safetensors,npcache,dummy,sharded_state,gguf,bitsandbytes,layered,flash_rl,remote,remote_instance,private}]
                        [--speculative-num-steps SPECULATIVE_NUM_STEPS]
                        [--speculative-eagle-topk SPECULATIVE_EAGLE_TOPK]
                        [--speculative-num-draft-tokens SPECULATIVE_NUM_DRAFT_TOKENS]
                        [--speculative-accept-threshold-single SPECULATIVE_ACCEPT_THRESHOLD_SINGLE]
                        [--speculative-accept-threshold-acc SPECULATIVE_ACCEPT_THRESHOLD_ACC]
                        [--speculative-token-map SPECULATIVE_TOKEN_MAP]
                        [--speculative-attention-mode {prefill,decode}]
                        [--speculative-draft-attention-backend SPECULATIVE_DRAFT_ATTENTION_BACKEND]
                        [--speculative-moe-runner-backend {auto,deep_gemm,triton,triton_kernel,flashinfer_trtllm,flashinfer_cutlass,flashinfer_mxfp4,flashinfer_cutedsl,cutlass}]
                        [--speculative-moe-a2a-backend {none,deepep,mooncake,ascend_fuseep}]
                        [--speculative-draft-model-quantization {awq,fp8,gptq,marlin,gptq_marlin,awq_marlin,bitsandbytes,gguf,modelopt,modelopt_fp8,modelopt_fp4,petit_nvfp4,w8a8_int8,w8a8_fp8,moe_wna16,qoq,w4afp8,mxfp4,auto-round,compressed-tensors,modelslim,unquant}]
                        [--speculative-ngram-min-match-window-size SPECULATIVE_NGRAM_MIN_MATCH_WINDOW_SIZE]
                        [--speculative-ngram-max-match-window-size SPECULATIVE_NGRAM_MAX_MATCH_WINDOW_SIZE]
                        [--speculative-ngram-min-bfs-breadth SPECULATIVE_NGRAM_MIN_BFS_BREADTH]
                        [--speculative-ngram-max-bfs-breadth SPECULATIVE_NGRAM_MAX_BFS_BREADTH]
                        [--speculative-ngram-match-type {BFS,PROB}]
                        [--speculative-ngram-branch-length SPECULATIVE_NGRAM_BRANCH_LENGTH]
                        [--speculative-ngram-capacity SPECULATIVE_NGRAM_CAPACITY]
                        [--enable-mtp]
                        [--expert-parallel-size EXPERT_PARALLEL_SIZE]
                        [--moe-a2a-backend {none,deepep,mooncake,ascend_fuseep}]
                        [--moe-runner-backend {auto,deep_gemm,triton,triton_kernel,flashinfer_trtllm,flashinfer_cutlass,flashinfer_mxfp4,flashinfer_cutedsl,cutlass}]
                        [--flashinfer-mxfp4-moe-precision {default,bf16}]
                        [--enable-flashinfer-allreduce-fusion]
                        [--deepep-mode {normal,low_latency,auto}]
                        [--ep-num-redundant-experts EP_NUM_REDUNDANT_EXPERTS]
                        [--ep-dispatch-algorithm EP_DISPATCH_ALGORITHM]
                        [--init-expert-location INIT_EXPERT_LOCATION]
                        [--enable-eplb] [--enable-async-eplb]
                        [--eplb-algorithm EPLB_ALGORITHM]
                        [--eplb-rebalance-num-iterations EPLB_REBALANCE_NUM_ITERATIONS]
                        [--eplb-rebalance-layers-per-chunk EPLB_REBALANCE_LAYERS_PER_CHUNK]
                        [--eplb-min-rebalancing-utilization-threshold EPLB_MIN_REBALANCING_UTILIZATION_THRESHOLD]
                        [--expert-distribution-recorder-mode EXPERT_DISTRIBUTION_RECORDER_MODE]
                        [--expert-distribution-recorder-buffer-size EXPERT_DISTRIBUTION_RECORDER_BUFFER_SIZE]
                        [--enable-expert-distribution-metrics]
                        [--deepep-config DEEPEP_CONFIG]
                        [--moe-dense-tp-size MOE_DENSE_TP_SIZE]
                        [--elastic-ep-backend {none,mooncake}]
                        [--mooncake-ib-device MOONCAKE_IB_DEVICE]
                        [--max-mamba-cache-size MAX_MAMBA_CACHE_SIZE]
                        [--mamba-ssm-dtype {float32,bfloat16}]
                        [--mamba-full-memory-ratio MAMBA_FULL_MEMORY_RATIO]
                        [--mamba-scheduler-strategy {auto,no_buffer,extra_buffer}]
                        [--mamba-track-interval MAMBA_TRACK_INTERVAL]
                        [--enable-hierarchical-cache]
                        [--hicache-ratio HICACHE_RATIO]
                        [--hicache-size HICACHE_SIZE]
                        [--hicache-write-policy {write_back,write_through,write_through_selective}]
                        [--hicache-io-backend {direct,kernel,kernel_ascend}]
                        [--hicache-mem-layout {layer_first,page_first,page_first_direct,page_first_kv_split,page_head}]
                        [--hicache-storage-backend {file,mooncake,hf3fs,nixl,aibrix,dynamic,eic}]
                        [--hicache-storage-prefetch-policy {best_effort,wait_complete,timeout}]
                        [--hicache-storage-backend-extra-config HICACHE_STORAGE_BACKEND_EXTRA_CONFIG]
                        [--enable-lmcache] [--kt-weight-path KT_WEIGHT_PATH]
                        [--kt-method KT_METHOD] [--kt-cpuinfer KT_CPUINFER]
                        [--kt-threadpool-count KT_THREADPOOL_COUNT]
                        [--kt-num-gpu-experts KT_NUM_GPU_EXPERTS]
                        [--kt-max-deferred-experts-per-token KT_MAX_DEFERRED_EXPERTS_PER_TOKEN]
                        [--dllm-algorithm DLLM_ALGORITHM]
                        [--dllm-algorithm-config DLLM_ALGORITHM_CONFIG]
                        [--enable-double-sparsity]
                        [--ds-channel-config-path DS_CHANNEL_CONFIG_PATH]
                        [--ds-heavy-channel-num DS_HEAVY_CHANNEL_NUM]
                        [--ds-heavy-token-num DS_HEAVY_TOKEN_NUM]
                        [--ds-heavy-channel-type DS_HEAVY_CHANNEL_TYPE]
                        [--ds-sparse-decode-threshold DS_SPARSE_DECODE_THRESHOLD]
                        [--cpu-offload-gb CPU_OFFLOAD_GB]
                        [--offload-group-size OFFLOAD_GROUP_SIZE]
                        [--offload-num-in-group OFFLOAD_NUM_IN_GROUP]
                        [--offload-prefetch-step OFFLOAD_PREFETCH_STEP]
                        [--offload-mode OFFLOAD_MODE]
                        [--multi-item-scoring-delimiter MULTI_ITEM_SCORING_DELIMITER]
                        [--disable-radix-cache]
                        [--cuda-graph-max-bs CUDA_GRAPH_MAX_BS]
                        [--cuda-graph-bs CUDA_GRAPH_BS [CUDA_GRAPH_BS ...]]
                        [--disable-cuda-graph] [--disable-cuda-graph-padding]
                        [--enable-profile-cuda-graph] [--enable-cudagraph-gc]
                        [--enable-layerwise-nvtx-marker] [--enable-nccl-nvls]
                        [--enable-symm-mem]
                        [--disable-flashinfer-cutlass-moe-fp4-allgather]
                        [--enable-tokenizer-batch-encode]
                        [--disable-tokenizer-batch-decode]
                        [--disable-outlines-disk-cache]
                        [--disable-custom-all-reduce] [--enable-mscclpp]
                        [--enable-torch-symm-mem] [--disable-overlap-schedule]
                        [--enable-mixed-chunk] [--enable-dp-attention]
                        [--enable-dp-lm-head] [--enable-two-batch-overlap]
                        [--enable-single-batch-overlap]
                        [--tbo-token-distribution-threshold TBO_TOKEN_DISTRIBUTION_THRESHOLD]
                        [--enable-torch-compile]
                        [--enable-torch-compile-debug-mode]
                        [--enable-piecewise-cuda-graph]
                        [--piecewise-cuda-graph-tokens PIECEWISE_CUDA_GRAPH_TOKENS]
                        [--piecewise-cuda-graph-compiler {eager,inductor}]
                        [--torch-compile-max-bs TORCH_COMPILE_MAX_BS]
                        [--piecewise-cuda-graph-max-tokens PIECEWISE_CUDA_GRAPH_MAX_TOKENS]
                        [--torchao-config TORCHAO_CONFIG]
                        [--enable-nan-detection] [--enable-p2p-check]
                        [--triton-attention-reduce-in-fp32]
                        [--triton-attention-num-kv-splits TRITON_ATTENTION_NUM_KV_SPLITS]
                        [--triton-attention-split-tile-size TRITON_ATTENTION_SPLIT_TILE_SIZE]
                        [--num-continuous-decode-steps NUM_CONTINUOUS_DECODE_STEPS]
                        [--delete-ckpt-after-loading] [--enable-memory-saver]
                        [--enable-weights-cpu-backup]
                        [--enable-draft-weights-cpu-backup]
                        [--allow-auto-truncate]
                        [--enable-custom-logit-processor]
                        [--flashinfer-mla-disable-ragged]
                        [--disable-shared-experts-fusion]
                        [--disable-chunked-prefix-cache]
                        [--disable-fast-image-processor]
                        [--keep-mm-feature-on-device]
                        [--enable-return-hidden-states]
                        [--enable-return-routed-experts]
                        [--scheduler-recv-interval SCHEDULER_RECV_INTERVAL]
                        [--numa-node NUMA_NODE [NUMA_NODE ...]]
                        [--enable-deterministic-inference]
                        [--rl-on-policy-target {fsdp}]
                        [--enable-attn-tp-input-scattered]
                        [--enable-nsa-prefill-context-parallel]
                        [--enable-fused-qk-norm-rope]
                        [--enable-dynamic-batch-tokenizer]
                        [--dynamic-batch-tokenizer-batch-size DYNAMIC_BATCH_TOKENIZER_BATCH_SIZE]
                        [--dynamic-batch-tokenizer-batch-timeout DYNAMIC_BATCH_TOKENIZER_BATCH_TIMEOUT]
                        [--debug-tensor-dump-output-folder DEBUG_TENSOR_DUMP_OUTPUT_FOLDER]
                        [--debug-tensor-dump-layers DEBUG_TENSOR_DUMP_LAYERS [DEBUG_TENSOR_DUMP_LAYERS ...]]
                        [--debug-tensor-dump-input-file DEBUG_TENSOR_DUMP_INPUT_FILE]
                        [--debug-tensor-dump-inject DEBUG_TENSOR_DUMP_INJECT]
                        [--disaggregation-mode {null,prefill,decode}]
                        [--disaggregation-transfer-backend {mooncake,nixl,ascend,fake}]
                        [--disaggregation-bootstrap-port DISAGGREGATION_BOOTSTRAP_PORT]
                        [--disaggregation-decode-tp DISAGGREGATION_DECODE_TP]
                        [--disaggregation-decode-dp DISAGGREGATION_DECODE_DP]
                        [--disaggregation-prefill-pp DISAGGREGATION_PREFILL_PP]
                        [--disaggregation-ib-device DISAGGREGATION_IB_DEVICE]
                        [--disaggregation-decode-enable-offload-kvcache]
                        [--num-reserved-decode-tokens NUM_RESERVED_DECODE_TOKENS]
                        [--disaggregation-decode-polling-interval DISAGGREGATION_DECODE_POLLING_INTERVAL]
                        [--encoder-only] [--language-only]
                        [--encoder-transfer-backend {zmq_to_scheduler,zmq_to_tokenizer,mooncake}]
                        [--encoder-urls ENCODER_URLS [ENCODER_URLS ...]]
                        [--custom-weight-loader [CUSTOM_WEIGHT_LOADER ...]]
                        [--weight-loader-disable-mmap]
                        [--remote-instance-weight-loader-seed-instance-ip REMOTE_INSTANCE_WEIGHT_LOADER_SEED_INSTANCE_IP]
                        [--remote-instance-weight-loader-seed-instance-service-port REMOTE_INSTANCE_WEIGHT_LOADER_SEED_INSTANCE_SERVICE_PORT]
                        [--remote-instance-weight-loader-send-weights-group-ports REMOTE_INSTANCE_WEIGHT_LOADER_SEND_WEIGHTS_GROUP_PORTS]
                        [--remote-instance-weight-loader-backend {transfer_engine,nccl}]
                        [--remote-instance-weight-loader-start-seed-via-transfer-engine]
                        [--enable-pdmux]
                        [--pdmux-config-path PDMUX_CONFIG_PATH]
                        [--sm-group-num SM_GROUP_NUM] [--config CONFIG]
                        [--mm-max-concurrent-calls MM_MAX_CONCURRENT_CALLS]
                        [--mm-per-request-timeout MM_PER_REQUEST_TIMEOUT]
                        [--enable-broadcast-mm-inputs-process]
                        [--mm-process-config MM_PROCESS_CONFIG]
                        [--mm-enable-dp-encoder]
                        [--decrypted-config-file DECRYPTED_CONFIG_FILE]
                        [--decrypted-draft-config-file DECRYPTED_DRAFT_CONFIG_FILE]
                        [--enable-prefix-mm-cache]
                        [--forward-hooks FORWARD_HOOKS]
launch_server.py: error: unrecognized arguments: 5
E/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2025-12-28 09:54:56] server_args=ServerArgs(model_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct', tokenizer_path='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct', tokenizer_mode='auto', tokenizer_worker_num=1, skip_tokenizer_init=False, load_format='auto', model_loader_extra_config='{}', trust_remote_code=False, context_length=None, is_embedding=False, enable_multimodal=None, limit_mm_data_per_request=None, revision=None, model_impl='auto', host='127.0.0.1', port=21000, fastapi_root_path='', grpc_mode=False, skip_server_warmup=False, warmups=None, nccl_port=None, checkpoint_engine_wait_weights_before_ready=False, dtype='auto', quantization=None, quantization_param_path=None, kv_cache_dtype='auto', enable_fp32_lm_head=False, modelopt_quant=None, modelopt_checkpoint_restore_path=None, modelopt_checkpoint_save_path=None, modelopt_export_path=None, quantize_and_serve=False, rl_quant_profile=None, mem_fraction_static=0.792, max_running_requests=None, max_queued_requests=None, max_total_tokens=None, chunked_prefill_size=8192, enable_dynamic_chunking=False, max_prefill_tokens=16384, prefill_max_requests=None, schedule_policy='fcfs', enable_priority_scheduling=False, abort_on_priority_when_disabled=False, schedule_low_priority_values_first=False, priority_scheduling_preemption_threshold=10, schedule_conservativeness=1.0, page_size=128, hybrid_kvcache_ratio=None, swa_full_tokens_ratio=0.8, disable_hybrid_swa_memory=False, radix_eviction_policy='lru', device='npu', tp_size=1, pp_size=1, pp_max_micro_batch_size=None, pp_async_batch_depth=0, stream_interval=1, stream_output=False, random_seed=386186250, constrained_json_whitespace_pattern=None, constrained_json_disable_any_whitespace=False, watchdog_timeout=300, soft_watchdog_timeout=None, dist_timeout=None, download_dir=None, base_gpu_id=0, gpu_id_step=1, sleep_on_idle=False, custom_sigquit_handler=None, log_level='info', log_level_http=None, log_requests=False, log_requests_level=2, crash_dump_folder=None, show_time_cost=False, enable_metrics=False, enable_metrics_for_all_schedulers=False, tokenizer_metrics_custom_labels_header='x-custom-labels', tokenizer_metrics_allowed_custom_labels=None, bucket_time_to_first_token=None, bucket_inter_token_latency=None, bucket_e2e_request_latency=None, collect_tokens_histogram=False, prompt_tokens_buckets=None, generation_tokens_buckets=None, gc_warning_threshold_secs=0.0, decode_log_interval=40, enable_request_time_stats_logging=False, kv_events_config=None, enable_trace=False, otlp_traces_endpoint='localhost:4317', export_metrics_to_file=False, export_metrics_to_file_dir=None, api_key='sk-123456', served_model_name='/data/ascend-ci-share-pkking-sglang/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct', weight_version='default', chat_template=None, completion_template=None, file_storage_path='sglang_storage', enable_cache_report=False, reasoning_parser=None, tool_call_parser=None, tool_server=None, sampling_defaults='model', dp_size=1, load_balance_method='round_robin', load_watch_interval=0.1, prefill_round_robin_balance=False, dist_init_addr=None, nnodes=1, node_rank=0, json_model_override_args='{}', preferred_sampling_params=None, enable_lora=None, max_lora_rank=None, lora_target_modules=None, lora_paths=None, max_loaded_loras=None, max_loras_per_batch=8, lora_eviction_policy='lru', lora_backend='csgmv', max_lora_chunk_size=16, attention_backend='ascend', decode_attention_backend='ascend', prefill_attention_backend='ascend', sampling_backend='pytorch', grammar_backend='xgrammar', mm_attention_backend=None, fp8_gemm_runner_backend='auto', nsa_prefill_backend='flashmla_sparse', nsa_decode_backend='fa3', disable_flashinfer_autotune=False, speculative_algorithm=None, speculative_draft_model_path=None, speculative_draft_model_revision=None, speculative_draft_load_format=None, speculative_num_steps=None, speculative_eagle_topk=None, speculative_num_draft_tokens=None, speculative_accept_threshold_single=1.0, speculative_accept_threshold_acc=1.0, speculative_token_map=None, speculative_attention_mode='prefill', speculative_draft_attention_backend=None, speculative_moe_runner_backend='auto', speculative_moe_a2a_backend=None, speculative_draft_model_quantization=None, speculative_ngram_min_match_window_size=1, speculative_ngram_max_match_window_size=12, speculative_ngram_min_bfs_breadth=1, speculative_ngram_max_bfs_breadth=10, speculative_ngram_match_type='BFS', speculative_ngram_branch_length=18, speculative_ngram_capacity=10000000, enable_mtp=False, ep_size=1, moe_a2a_backend='none', moe_runner_backend='auto', flashinfer_mxfp4_moe_precision='default', enable_flashinfer_allreduce_fusion=False, deepep_mode='auto', ep_num_redundant_experts=0, ep_dispatch_algorithm=None, init_expert_location='trivial', enable_eplb=False, enable_async_eplb=False, eplb_algorithm='auto', eplb_rebalance_num_iterations=1000, eplb_rebalance_layers_per_chunk=None, eplb_min_rebalancing_utilization_threshold=1.0, expert_distribution_recorder_mode=None, expert_distribution_recorder_buffer_size=1000, enable_expert_distribution_metrics=False, deepep_config=None, moe_dense_tp_size=None, elastic_ep_backend=None, mooncake_ib_device=None, max_mamba_cache_size=None, mamba_ssm_dtype='float32', mamba_full_memory_ratio=0.9, mamba_scheduler_strategy='no_buffer', mamba_track_interval=256, enable_hierarchical_cache=False, hicache_ratio=2.0, hicache_size=0, hicache_write_policy='write_through', hicache_io_backend='kernel', hicache_mem_layout='layer_first', hicache_storage_backend=None, hicache_storage_prefetch_policy='best_effort', hicache_storage_backend_extra_config=None, enable_lmcache=False, kt_weight_path=None, kt_method='AMXINT4', kt_cpuinfer=None, kt_threadpool_count=2, kt_num_gpu_experts=None, kt_max_deferred_experts_per_token=None, dllm_algorithm=None, dllm_algorithm_config=None, enable_double_sparsity=False, ds_channel_config_path=None, ds_heavy_channel_num=32, ds_heavy_token_num=256, ds_heavy_channel_type='qk', ds_sparse_decode_threshold=4096, cpu_offload_gb=0, offload_group_size=-1, offload_num_in_group=1, offload_prefetch_step=1, offload_mode='cpu', multi_item_scoring_delimiter=None, disable_radix_cache=False, cuda_graph_max_bs=64, cuda_graph_bs=[1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], disable_cuda_graph=False, disable_cuda_graph_padding=False, enable_profile_cuda_graph=False, enable_cudagraph_gc=False, enable_layerwise_nvtx_marker=False, enable_nccl_nvls=False, enable_symm_mem=False, disable_flashinfer_cutlass_moe_fp4_allgather=False, enable_tokenizer_batch_encode=False, disable_tokenizer_batch_decode=False, disable_outlines_disk_cache=False, disable_custom_all_reduce=True, enable_mscclpp=False, enable_torch_symm_mem=False, disable_overlap_schedule=False, enable_mixed_chunk=False, enable_dp_attention=False, enable_dp_lm_head=False, enable_two_batch_overlap=False, enable_single_batch_overlap=False, tbo_token_distribution_threshold=0.48, enable_torch_compile=False, enable_piecewise_cuda_graph=False, enable_torch_compile_debug_mode=False, torch_compile_max_bs=32, piecewise_cuda_graph_max_tokens=4096, piecewise_cuda_graph_tokens=[4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], piecewise_cuda_graph_compiler='eager', torchao_config='', enable_nan_detection=False, enable_p2p_check=False, triton_attention_reduce_in_fp32=False, triton_attention_num_kv_splits=8, triton_attention_split_tile_size=None, num_continuous_decode_steps=1, delete_ckpt_after_loading=False, enable_memory_saver=False, enable_weights_cpu_backup=False, enable_draft_weights_cpu_backup=False, allow_auto_truncate=False, enable_custom_logit_processor=False, flashinfer_mla_disable_ragged=False, disable_shared_experts_fusion=False, disable_chunked_prefix_cache=False, disable_fast_image_processor=False, keep_mm_feature_on_device=False, enable_return_hidden_states=True, enable_return_routed_experts=False, scheduler_recv_interval=1, numa_node=None, enable_deterministic_inference=False, rl_on_policy_target=None, enable_attn_tp_input_scattered=False, enable_nsa_prefill_context_parallel=False, enable_fused_qk_norm_rope=False, enable_dynamic_batch_tokenizer=False, dynamic_batch_tokenizer_batch_size=32, dynamic_batch_tokenizer_batch_timeout=0.002, debug_tensor_dump_output_folder=None, debug_tensor_dump_layers=None, debug_tensor_dump_input_file=None, debug_tensor_dump_inject=False, disaggregation_mode='null', disaggregation_transfer_backend='mooncake', disaggregation_bootstrap_port=8998, disaggregation_decode_tp=None, disaggregation_decode_dp=None, disaggregation_prefill_pp=1, disaggregation_ib_device=None, disaggregation_decode_enable_offload_kvcache=False, num_reserved_decode_tokens=512, disaggregation_decode_polling_interval=1, encoder_only=False, language_only=False, encoder_transfer_backend='zmq_to_scheduler', encoder_urls=[], custom_weight_loader=[], weight_loader_disable_mmap=False, remote_instance_weight_loader_seed_instance_ip=None, remote_instance_weight_loader_seed_instance_service_port=None, remote_instance_weight_loader_send_weights_group_ports=None, remote_instance_weight_loader_backend='nccl', remote_instance_weight_loader_start_seed_via_transfer_engine=False, enable_pdmux=False, pdmux_config_path=None, sm_group_num=8, mm_max_concurrent_calls=32, mm_per_request_timeout=10.0, enable_broadcast_mm_inputs_process=False, enable_prefix_mm_cache=False, mm_enable_dp_encoder=False, mm_process_config={}, decrypted_config_file=None, decrypted_draft_config_file=None, forward_hooks=None)
[2025-12-28 09:54:56] Using default HuggingFace chat template with detected content format: string
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/awq.py:77: UserWarning: Only CUDA, HIP and XPU support AWQ currently.
  warnings.warn(f"Only CUDA, HIP and XPU support AWQ currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/layers/quantization/gguf.py:46: UserWarning: Only CUDA support GGUF quantization currently.
  warnings.warn(f"Only CUDA support GGUF quantization currently.")
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:347: ImportWarning: 
    *************************************************************************************************************
    The torch.Tensor.cuda and torch.nn.Module.cuda are replaced with torch.Tensor.npu and torch.nn.Module.npu now..
    The torch.cuda.DoubleTensor is replaced with torch.npu.FloatTensor cause the double type is not supported now..
    The backend in torch.distributed.init_process_group set to hccl now..
    The torch.cuda.* and torch.cuda.amp.* are replaced with torch.npu.* and torch.npu.amp.* now..
    The device parameters have been replaced with npu in the function below:
    torch.logspace, torch.randint, torch.hann_window, torch.rand, torch.full_like, torch.ones_like, torch.rand_like, torch.randperm, torch.arange, torch.frombuffer, torch.normal, torch._empty_per_channel_affine_quantized, torch.empty_strided, torch.empty_like, torch.scalar_tensor, torch.tril_indices, torch.bartlett_window, torch.ones, torch.sparse_coo_tensor, torch.randn, torch.kaiser_window, torch.tensor, torch.triu_indices, torch.as_tensor, torch.zeros, torch.randint_like, torch.full, torch.eye, torch._sparse_csr_tensor_unsafe, torch.empty, torch._sparse_coo_tensor_unsafe, torch.blackman_window, torch.zeros_like, torch.range, torch.sparse_csr_tensor, torch.randn_like, torch.from_file, torch._cudnn_init_dropout_state, torch._empty_affine_quantized, torch.linspace, torch.hamming_window, torch.empty_quantized, torch._pin_memory, torch.load, torch.set_default_device, torch.get_device_module, torch.sparse_compressed_tensor, torch.Tensor.new_empty, torch.Tensor.new_empty_strided, torch.Tensor.new_full, torch.Tensor.new_ones, torch.Tensor.new_tensor, torch.Tensor.new_zeros, torch.Tensor.to, torch.Tensor.pin_memory, torch.nn.Module.to, torch.nn.Module.to_empty
    *************************************************************************************************************
    
  warnings.warn(msg, ImportWarning)
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/contrib/transfer_to_npu.py:276: RuntimeWarning: torch.jit.script and torch.jit.script_method will be disabled by transfer_to_npu, which currently does not support them, if you need to enable them, please do not use transfer_to_npu.
  warnings.warn(msg, RuntimeWarning)
[2025-12-28 09:55:07] Init torch distributed begin.
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[2025-12-28 09:55:08] Init torch distributed ends. mem usage=0.00 GB
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2025-12-28 09:55:08] MOE_RUNNER_BACKEND is not initialized, the backend will be automatically selected
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2025-12-28 09:55:09] Ignore import error when loading sglang.srt.models.midashenglm: No module named 'torchaudio'
[2025-12-28 09:55:09] Ignore import error when loading sglang.srt.models.mindspore: No module named 'mindspore'
/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/dynamo/torchair/configs/compiler_config.py:106: UserWarning: The following torchair config or properties may not take effect or report error in max-autotune mode: config.debug.aclgraph.clone_input:True
  warnings.warn(
[2025-12-28 09:55:09] Load weight begin. avail mem=60.83 GB
Loading safetensors checkpoint shards:   0% Completed | 0/1 [00:00<?, ?it/s]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.68s/it]
Loading safetensors checkpoint shards: 100% Completed | 1/1 [00:02<00:00,  2.68s/it]

[2025-12-28 09:55:12] Load weight end. type=LlamaForCausalLM, dtype=torch.bfloat16, avail mem=58.37 GB, mem usage=2.46 GB.
[2025-12-28 09:55:12] Using KV cache dtype: torch.bfloat16
[2025-12-28 09:55:12] The available memory for KV cache is 45.70 GB.
[2025-12-28 09:55:12] KV Cache is allocated. #tokens: 1497600, K size: 22.85 GB, V size: 22.85 GB
[2025-12-28 09:55:12] Memory pool end. avail mem=10.65 GB
[2025-12-28 09:55:13] Capture cuda graph begin. This can take up to several minutes. avail mem=10.65 GB
[2025-12-28 09:55:13] Capture cuda graph bs [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64]
  0%|          | 0/12 [00:00<?, ?it/s]Capturing batches (bs=64 avail_mem=10.60 GB):   0%|          | 0/12 [00:00<?, ?it/s]/usr/local/python3.11.13/lib/python3.11/site-packages/torch_npu/utils/storage.py:90: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()
  if self.device.type != 'cpu':
Capturing batches (bs=64 avail_mem=10.60 GB):   8%|▊         | 1/12 [00:01<00:11,  1.09s/it]Capturing batches (bs=56 avail_mem=10.34 GB):   8%|▊         | 1/12 [00:01<00:11,  1.09s/it]Capturing batches (bs=48 avail_mem=10.32 GB):   8%|▊         | 1/12 [00:01<00:11,  1.09s/it]Capturing batches (bs=48 avail_mem=10.32 GB):  25%|██▌       | 3/12 [00:01<00:02,  3.10it/s]Capturing batches (bs=40 avail_mem=10.32 GB):  25%|██▌       | 3/12 [00:01<00:02,  3.10it/s]Capturing batches (bs=32 avail_mem=10.32 GB):  25%|██▌       | 3/12 [00:01<00:02,  3.10it/s]Capturing batches (bs=32 avail_mem=10.32 GB):  42%|████▏     | 5/12 [00:01<00:01,  5.42it/s]Capturing batches (bs=24 avail_mem=10.32 GB):  42%|████▏     | 5/12 [00:01<00:01,  5.42it/s]Capturing batches (bs=16 avail_mem=10.32 GB):  42%|████▏     | 5/12 [00:01<00:01,  5.42it/s]Capturing batches (bs=16 avail_mem=10.32 GB):  58%|█████▊    | 7/12 [00:01<00:00,  7.75it/s]Capturing batches (bs=12 avail_mem=10.32 GB):  58%|█████▊    | 7/12 [00:01<00:00,  7.75it/s]Capturing batches (bs=8 avail_mem=10.32 GB):  58%|█████▊    | 7/12 [00:01<00:00,  7.75it/s] Capturing batches (bs=8 avail_mem=10.32 GB):  75%|███████▌  | 9/12 [00:01<00:00,  9.93it/s]Capturing batches (bs=4 avail_mem=10.32 GB):  75%|███████▌  | 9/12 [00:01<00:00,  9.93it/s]Capturing batches (bs=2 avail_mem=10.32 GB):  75%|███████▌  | 9/12 [00:01<00:00,  9.93it/s]Capturing batches (bs=2 avail_mem=10.32 GB):  92%|█████████▏| 11/12 [00:01<00:00, 11.86it/s]Capturing batches (bs=1 avail_mem=10.32 GB):  92%|█████████▏| 11/12 [00:01<00:00, 11.86it/s]Capturing batches (bs=1 avail_mem=10.32 GB): 100%|██████████| 12/12 [00:01<00:00,  7.15it/s]
[2025-12-28 09:55:15] Capture cuda graph end. Time elapsed: 2.56 s. mem usage=0.33 GB. avail mem=10.31 GB.
/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py:1204: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at /pytorch/torch/csrc/utils/tensor_numpy.cpp:203.)
  tensor_data = torch.ByteTensor(
[2025-12-28 09:55:15] max_total_num_tokens=1497600, chunked_prefill_size=8192, max_prefill_tokens=16384, max_running_requests=4096, context_len=131072, available_gpu_mem=10.31 GB
[2025-12-28 09:55:16] INFO:     Started server process [48735]
[2025-12-28 09:55:16] INFO:     Waiting for application startup.
[2025-12-28 09:55:16] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2025-12-28 09:55:16] Using default chat sampling params from model generation config: {'repetition_penalty': 1.0, 'temperature': 0.6, 'top_k': 50, 'top_p': 0.9}
[2025-12-28 09:55:16] INFO:     Application startup complete.
[2025-12-28 09:55:16] INFO:     Uvicorn running on http://127.0.0.1:21000 (Press CTRL+C to quit)
[2025-12-28 09:55:17] INFO:     127.0.0.1:59184 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2025-12-28 09:55:17] INFO:     127.0.0.1:59194 - "GET /model_info HTTP/1.1" 200 OK
[2025-12-28 09:55:17] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[rank0]:[W1228 09:55:17.662564329 compiler_depend.ts:198] Warning: Driver Version: "ચ" is invalid or not supported yet. (function operator())
[2025-12-28 09:55:27] INFO:     127.0.0.1:41316 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2025-12-28 09:55:37] INFO:     127.0.0.1:49262 - "GET /health_generate HTTP/1.1" 503 Service Unavailable
[2025-12-28 09:55:38] INFO:     127.0.0.1:59200 - "POST /generate HTTP/1.1" 200 OK
[2025-12-28 09:55:38] The server is fired up and ready to roll!
[2025-12-28 09:55:47] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-28 09:55:48] INFO:     127.0.0.1:57306 - "GET /health_generate HTTP/1.1" 200 OK
[2025-12-28 09:55:48] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 0, #queue-req: 0, 
[2025-12-28 09:55:48] Prefill batch, #new-seq: 1, #new-token: 128, #cached-token: 0, token usage: 0.00, #running-req: 1, #queue-req: 0, 
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
[ERROR] TBE Subprocess[task_distribute] raise error[], main process disappeared!
rm: cannot remove '/data/l30079981/run_sglang_debug/run_sglang/1224/ascend/function/kernel_meta/kernel_meta_temp_5558878239522379265': Directory not empty
[2025-12-28 10:17:35] INFO:     Shutting down
[2025-12-28 10:17:35] INFO:     Waiting for connections to close. (CTRL+C to force quit)
Process Process-2:
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/multiprocessing/process.py", line 314, in _bootstrap
    self.run()
  File "/usr/local/python3.11.13/lib/python3.11/multiprocessing/process.py", line 108, in run
    self._target(*self._args, **self._kwargs)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/detokenizer_manager.py", line 362, in run_detokenizer_process
    manager.event_loop()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/managers/detokenizer_manager.py", line 117, in event_loop
    recv_obj = self.recv_from_scheduler.recv_pyobj()
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/zmq/sugar/socket.py", line 989, in recv_pyobj
    msg = self.recv(flags)
          ^^^^^^^^^^^^^^^^
  File "zmq/backend/cython/_zmq.py", line 1218, in zmq.backend.cython._zmq.Socket.recv
  File "zmq/backend/cython/_zmq.py", line 1253, in zmq.backend.cython._zmq.Socket.recv
  File "zmq/backend/cython/_zmq.py", line 1408, in zmq.backend.cython._zmq._recv_copy
  File "zmq/backend/cython/_zmq.py", line 179, in zmq.backend.cython._zmq._check_rc
KeyboardInterrupt
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/LLM-Research/Llama-3.1-8B-Instruct/ --speculative-algorithm EAGLE3 --speculative-draft-model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/sglang-EAGLE3-LLaMA3.1-Instruct-8B --speculative-num-steps 5 --speculative-eagle-topk 16 --speculative-num-draft-tokens 64 --mem-fraction-static 0.7 --chunked-prefill-size 128 --max-running-requests 8 --dtype float16 --enable-return-hidden-states --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/NousResearch/Llama-2-7b-chat-hf --speculative-algorithm EAGLE --speculative-draft-model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/sglang-EAGLE-llama2-chat-7B--speculative-num-steps 5 --speculative-eagle-topk 8 --speculative-num-draft-tokens 64 --mem-fraction-static 0.7 --chunked-prefill-size 128 --max-running-requests 8 --enable-return-hidden-states --attention-backend ascend --disable-cuda-graph --device npu --host 127.0.0.1 --port 21000
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct --enable-return-hidden-states --device npu --host 127.0.0.1 --port 21000 --api-key sk-123456
[CI Test Method] TestOpenAIServerWithHiddenStatesEnabled.test_completion
----------------------testcase1
Traceback (most recent call last):
  File "/data/l30079981/run_sglang_debug/run_sglang/1224/ascend/function/test_openai_server_hidden_states.py", line 375, in <module>
    unittest.main()
  File "/usr/local/python3.11.13/lib/python3.11/unittest/main.py", line 102, in __init__
    self.runTests()
  File "/usr/local/python3.11.13/lib/python3.11/unittest/main.py", line 274, in runTests
    self.result = testRunner.run(self.test)
                  ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/runner.py", line 217, in run
    test(result)
  File "/usr/local/python3.11.13/lib/python3.11/unittest/suite.py", line 84, in __call__
    return self.run(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/suite.py", line 122, in run
    test(result)
  File "/usr/local/python3.11.13/lib/python3.11/unittest/suite.py", line 84, in __call__
    return self.run(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/suite.py", line 122, in run
    test(result)
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 678, in __call__
    return self.run(*args, **kwds)
           ^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 623, in run
    self._callTestMethod(testMethod)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1703, in _callTestMethod
    retry(
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 2527, in retry
    return fn()
           ^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/test/test_utils.py", line 1704, in <lambda>
    lambda: super(CustomTestCase, self)._callTestMethod(method),
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/unittest/case.py", line 579, in _callTestMethod
    if method() is not None:
       ^^^^^^^^
  File "/data/l30079981/run_sglang_debug/run_sglang/1224/ascend/function/test_openai_server_hidden_states.py", line 38, in test_completion
    self.run_completion(
  File "/data/l30079981/run_sglang_debug/run_sglang/1224/ascend/function/test_openai_server_hidden_states.py", line 95, in run_completion
    response = client.completions.create(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/openai/resources/completions.py", line 541, in create
    return self._post(
           ^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/openai/_base_client.py", line 982, in request
    response = self._client.send(
               ^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/httpx/_client.py", line 914, in send
    response = self._send_handling_auth(
               ^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/httpx/_client.py", line 942, in _send_handling_auth
    response = self._send_handling_redirects(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/httpx/_client.py", line 979, in _send_handling_redirects
    response = self._send_single_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/httpx/_client.py", line 1014, in _send_single_request
    response = transport.handle_request(request)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/httpx/_transports/default.py", line 250, in handle_request
    resp = self._pool.handle_request(req)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 256, in handle_request
    raise exc from None
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/httpcore/_sync/connection_pool.py", line 236, in handle_request
    response = connection.handle_request(
               ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/httpcore/_sync/connection.py", line 103, in handle_request
    return self._connection.handle_request(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 136, in handle_request
    raise exc
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 106, in handle_request
    ) = self._receive_response_headers(**kwargs)
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 177, in _receive_response_headers
    event = self._receive_event(timeout=timeout)
            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/httpcore/_sync/http11.py", line 217, in _receive_event
    data = self._network_stream.read(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/httpcore/_backends/sync.py", line 128, in read
    return self._sock.recv(max_bytes)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
KeyboardInterrupt
[2025-12-28 10:17:35] INFO:     Waiting for background tasks to complete. (CTRL+C to force quit)
[2025-12-28 10:17:37] INFO:     Finished server process [48735]
[2025-12-28 10:17:37] ERROR:    Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/uvicorn/_compat.py", line 30, in asyncio_run
    return runner.run(main)
           ^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/asyncio/runners.py", line 118, in run
    return self._loop.run_until_complete(task)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "uvloop/loop.pyx", line 1512, in uvloop.loop.Loop.run_until_complete
  File "uvloop/loop.pyx", line 1505, in uvloop.loop.Loop.run_until_complete
  File "uvloop/loop.pyx", line 1379, in uvloop.loop.Loop.run_forever
  File "uvloop/loop.pyx", line 557, in uvloop.loop.Loop._run
  File "uvloop/loop.pyx", line 476, in uvloop.loop.Loop._on_idle
  File "uvloop/cbhandles.pyx", line 83, in uvloop.loop.Handle._run
  File "uvloop/cbhandles.pyx", line 63, in uvloop.loop.Handle._run
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/uvicorn/server.py", line 70, in serve
    with self.capture_signals():
  File "/usr/local/python3.11.13/lib/python3.11/contextlib.py", line 144, in __exit__
    next(self.gen)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/uvicorn/server.py", line 331, in capture_signals
    signal.raise_signal(captured_signal)
  File "/usr/local/python3.11.13/lib/python3.11/asyncio/runners.py", line 157, in _on_sigint
    raise KeyboardInterrupt()
KeyboardInterrupt

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/starlette/routing.py", line 701, in lifespan
    await receive()
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/uvicorn/lifespan/on.py", line 137, in receive
    return await self.receive_queue.get()
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/asyncio/queues.py", line 158, in get
    await getter
asyncio.exceptions.CancelledError

[2025-12-28 10:17:37] ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/anyio/streams/memory.py", line 117, in receive
    return self.receive_nowait()
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/anyio/streams/memory.py", line 112, in receive_nowait
    raise WouldBlock
anyio.WouldBlock

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py", line 410, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/fastapi/applications.py", line 1135, in __call__
    await super().__call__(scope, receive, send)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/starlette/middleware/base.py", line 192, in __call__
    async with anyio.create_task_group() as task_group:
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/anyio/_backends/_asyncio.py", line 787, in __aexit__
    raise exc_val
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/starlette/middleware/base.py", line 193, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 1126, in authentication
    return await call_next(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/starlette/middleware/base.py", line 151, in call_next
    message = await recv_stream.receive()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/anyio/streams/memory.py", line 125, in receive
    await receive_event.wait()
  File "/usr/local/python3.11.13/lib/python3.11/asyncio/locks.py", line 213, in wait
    await fut
asyncio.exceptions.CancelledError
[2025-12-28 10:17:38] ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/anyio/streams/memory.py", line 117, in receive
    return self.receive_nowait()
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/anyio/streams/memory.py", line 112, in receive_nowait
    raise WouldBlock
anyio.WouldBlock

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py", line 410, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/fastapi/applications.py", line 1135, in __call__
    await super().__call__(scope, receive, send)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/starlette/middleware/base.py", line 192, in __call__
    async with anyio.create_task_group() as task_group:
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/anyio/_backends/_asyncio.py", line 787, in __aexit__
    raise exc_val
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/starlette/middleware/base.py", line 193, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 1126, in authentication
    return await call_next(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/starlette/middleware/base.py", line 151, in call_next
    message = await recv_stream.receive()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/anyio/streams/memory.py", line 125, in receive
    await receive_event.wait()
  File "/usr/local/python3.11.13/lib/python3.11/asyncio/locks.py", line 213, in wait
    await fut
asyncio.exceptions.CancelledError
[2025-12-28 10:17:38] ERROR:    Exception in ASGI application
Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/anyio/streams/memory.py", line 117, in receive
    return self.receive_nowait()
           ^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/anyio/streams/memory.py", line 112, in receive_nowait
    raise WouldBlock
anyio.WouldBlock

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/uvicorn/protocols/http/h11_impl.py", line 410, in run_asgi
    result = await app(  # type: ignore[func-returns-value]
             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/uvicorn/middleware/proxy_headers.py", line 60, in __call__
    return await self.app(scope, receive, send)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/fastapi/applications.py", line 1135, in __call__
    await super().__call__(scope, receive, send)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/starlette/applications.py", line 107, in __call__
    await self.middleware_stack(scope, receive, send)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/starlette/middleware/errors.py", line 164, in __call__
    await self.app(scope, receive, _send)
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/starlette/middleware/base.py", line 192, in __call__
    async with anyio.create_task_group() as task_group:
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/anyio/_backends/_asyncio.py", line 787, in __aexit__
    raise exc_val
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/starlette/middleware/base.py", line 193, in __call__
    response = await self.dispatch_func(request, call_next)
               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/sglang/srt/utils/common.py", line 1126, in authentication
    return await call_next(request)
           ^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/starlette/middleware/base.py", line 151, in call_next
    message = await recv_stream.receive()
              ^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/usr/local/python3.11.13/lib/python3.11/site-packages/anyio/streams/memory.py", line 125, in receive
    await receive_event.wait()
  File "/usr/local/python3.11.13/lib/python3.11/asyncio/locks.py", line 213, in wait
    await fut
asyncio.exceptions.CancelledError
