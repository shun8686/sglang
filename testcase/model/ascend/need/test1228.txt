[CI Test Method] TestNoChunkedPrefill.test_no_chunked_prefill_without_radix_cache
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-32B --trust-remote-code --tp-size 4 --mem-fraction-static 0.8 --max-running-requests 16 --disable-radix-cache --chunked-prefill-size -1 --disable-cuda-graph --base-gpu-id 8 --attention-backend ascend --device npu --host 127.0.0.1 --port 21000
benchmark_args=namespace(backend='sglang', base_url='http://127.0.0.1:21000', host=None, port=None, dataset_name='generated-shared-prefix', dataset_path='', model=None, tokenizer=None, num_prompts=128, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=3584, random_output_len=1, random_range_ratio=0.0, request_rate=inf, multi=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, return_routed_experts=False, seed=0, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=None, lora_name=None, lora_request_distribution='uniform', lora_zipf_alpha=1.5, prompt_suffix='', device='npu', pd_separated=False, gsp_num_groups=1, gsp_prompts_per_group=128, gsp_system_prompt_len=1792, gsp_question_len=1792, gsp_output_len=1, max_concurrency=16, warmup_requests=1, output_details=False, tokenize_prompt=False, plot_throughput=False, use_trace_timestamps=False, mooncake_slowdown_factor=1.0, mooncake_num_rounds=1, served_model_name=None)
namespace(backend='sglang', base_url='http://127.0.0.1:21000', host=None, port=30000, dataset_name='generated-shared-prefix', dataset_path='', model='/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct', tokenizer=None, num_prompts=128, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=3584, random_output_len=1, random_range_ratio=0.0, request_rate=inf, multi=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, return_routed_experts=False, seed=0, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=None, lora_name=None, lora_request_distribution='uniform', lora_zipf_alpha=1.5, prompt_suffix='', device='npu', pd_separated=False, gsp_num_groups=1, gsp_prompts_per_group=128, gsp_system_prompt_len=1792, gsp_question_len=1792, gsp_output_len=1, max_concurrency=16, warmup_requests=1, output_details=False, tokenize_prompt=False, plot_throughput=False, use_trace_timestamps=False, mooncake_slowdown_factor=1.0, mooncake_num_rounds=1, served_model_name=None)


Generating new input data... (num_groups=1, 128, system_prompt_len=1792, question_len=1792, output_len=1, range_ratio=1.0)

============ Serving Benchmark Result ============
Backend:                                 sglang    
Traffic request rate:                    inf       
Max request concurrency:                 16        
Successful requests:                     128       
Benchmark duration (s):                  601.90    
Total input tokens:                      481256    
Total input text tokens:                 481256    
Total input vision tokens:               0         
Total generated tokens:                  128       
Total generated tokens (retokenized):    127       
Request throughput (req/s):              0.21      
Input token throughput (tok/s):          799.57    
Output token throughput (tok/s):         0.21      
Peak output token throughput (tok/s):    8.00      
Peak concurrent requests:                20        
Total token throughput (tok/s):          799.78    
Concurrency:                             15.67     
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   73687.84  
Median E2E Latency (ms):                 75114.98  
---------------Time to First Token----------------
Mean TTFT (ms):                          73126.03  
Median TTFT (ms):                        75114.97  
P99 TTFT (ms):                           98016.11  
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          0.00      
Median TPOT (ms):                        0.00      
P99 TPOT (ms):                           0.00      
---------------Inter-Token Latency----------------
Mean ITL (ms):                           0.00      
Median ITL (ms):                         0.00      
P95 ITL (ms):                            0.00      
P99 ITL (ms):                            0.00      
Max ITL (ms):                            0.00      
==================================================
--------------------------------------------- {'tag': None, 'backend': 'sglang', 'dataset_name': 'generated-shared-prefix', 'request_rate': inf, 'max_concurrency': 16, 'sharegpt_output_len': None, 'random_input_len': 3584, 'random_output_len': 1, 'random_range_ratio': 0.0, 'server_info': {'model_path': '/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct', 'tokenizer_path': '/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct', 'tokenizer_mode': 'auto', 'tokenizer_worker_num': 1, 'skip_tokenizer_init': False, 'load_format': 'auto', 'model_loader_extra_config': '{}', 'trust_remote_code': False, 'context_length': None, 'is_embedding': False, 'enable_multimodal': None, 'limit_mm_data_per_request': None, 'revision': None, 'model_impl': 'auto', 'host': '127.0.0.1', 'port': 21000, 'fastapi_root_path': '', 'grpc_mode': False, 'skip_server_warmup': True, 'warmups': None, 'nccl_port': None, 'checkpoint_engine_wait_weights_before_ready': False, 'dtype': 'auto', 'quantization': None, 'quantization_param_path': None, 'kv_cache_dtype': 'auto', 'enable_fp32_lm_head': False, 'modelopt_quant': None, 'modelopt_checkpoint_restore_path': None, 'modelopt_checkpoint_save_path': None, 'modelopt_export_path': None, 'quantize_and_serve': False, 'rl_quant_profile': None, 'mem_fraction_static': 0.792, 'max_running_requests': None, 'max_queued_requests': None, 'max_total_tokens': None, 'chunked_prefill_size': 8192, 'enable_dynamic_chunking': False, 'max_prefill_tokens': 16384, 'prefill_max_requests': None, 'schedule_policy': 'fcfs', 'enable_priority_scheduling': False, 'abort_on_priority_when_disabled': False, 'schedule_low_priority_values_first': False, 'priority_scheduling_preemption_threshold': 10, 'schedule_conservativeness': 1.0, 'page_size': 128, 'hybrid_kvcache_ratio': None, 'swa_full_tokens_ratio': 0.8, 'disable_hybrid_swa_memory': False, 'radix_eviction_policy': 'lru', 'device': 'npu', 'tp_size': 1, 'pp_size': 1, 'pp_max_micro_batch_size': None, 'pp_async_batch_depth': 0, 'stream_interval': 1, 'stream_output': False, 'random_seed': 443814053, 'constrained_json_whitespace_pattern': None, 'constrained_json_disable_any_whitespace': False, 'watchdog_timeout': 300, 'soft_watchdog_timeout': None, 'dist_timeout': None, 'download_dir': None, 'base_gpu_id': 0, 'gpu_id_step': 1, 'sleep_on_idle': False, 'custom_sigquit_handler': None, 'log_level': 'info', 'log_level_http': None, 'log_requests': False, 'log_requests_level': 2, 'crash_dump_folder': None, 'show_time_cost': False, 'enable_metrics': False, 'enable_metrics_for_all_schedulers': False, 'tokenizer_metrics_custom_labels_header': 'x-custom-labels', 'tokenizer_metrics_allowed_custom_labels': None, 'bucket_time_to_first_token': None, 'bucket_inter_token_latency': None, 'bucket_e2e_request_latency': None, 'collect_tokens_histogram': False, 'prompt_tokens_buckets': None, 'generation_tokens_buckets': None, 'gc_warning_threshold_secs': 0.0, 'decode_log_interval': 40, 'enable_request_time_stats_logging': False, 'kv_events_config': None, 'enable_trace': False, 'otlp_traces_endpoint': 'localhost:4317', 'export_metrics_to_file': False, 'export_metrics_to_file_dir': None, 'api_key': None, 'served_model_name': '/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct', 'weight_version': 'default', 'chat_template': None, 'completion_template': None, 'file_storage_path': 'sglang_storage', 'enable_cache_report': False, 'reasoning_parser': None, 'tool_call_parser': None, 'tool_server': None, 'sampling_defaults': 'model', 'dp_size': 1, 'load_balance_method': 'round_robin', 'load_watch_interval': 0.1, 'prefill_round_robin_balance': False, 'dist_init_addr': None, 'nnodes': 1, 'node_rank': 0, 'json_model_override_args': '{}', 'preferred_sampling_params': None, 'enable_lora': None, 'max_lora_rank': None, 'lora_target_modules': None, 'lora_paths': None, 'max_loaded_loras': None, 'max_loras_per_batch': 8, 'lora_eviction_policy': 'lru', 'lora_backend': 'csgmv', 'max_lora_chunk_size': 16, 'attention_backend': 'ascend', 'decode_attention_backend': 'ascend', 'prefill_attention_backend': 'ascend', 'sampling_backend': 'pytorch', 'grammar_backend': 'xgrammar', 'mm_attention_backend': None, 'fp8_gemm_runner_backend': 'auto', 'nsa_prefill_backend': 'flashmla_sparse', 'nsa_decode_backend': 'fa3', 'disable_flashinfer_autotune': False, 'speculative_algorithm': None, 'speculative_draft_model_path': None, 'speculative_draft_model_revision': None, 'speculative_draft_load_format': None, 'speculative_num_steps': None, 'speculative_eagle_topk': None, 'speculative_num_draft_tokens': None, 'speculative_accept_threshold_single': 1.0, 'speculative_accept_threshold_acc': 1.0, 'speculative_token_map': None, 'speculative_attention_mode': 'prefill', 'speculative_draft_attention_backend': None, 'speculative_moe_runner_backend': 'auto', 'speculative_moe_a2a_backend': None, 'speculative_draft_model_quantization': None, 'speculative_ngram_min_match_window_size': 1, 'speculative_ngram_max_match_window_size': 12, 'speculative_ngram_min_bfs_breadth': 1, 'speculative_ngram_max_bfs_breadth': 10, 'speculative_ngram_match_type': 'BFS', 'speculative_ngram_branch_length': 18, 'speculative_ngram_capacity': 10000000, 'enable_mtp': False, 'ep_size': 1, 'moe_a2a_backend': 'none', 'moe_runner_backend': 'auto', 'flashinfer_mxfp4_moe_precision': 'default', 'enable_flashinfer_allreduce_fusion': False, 'deepep_mode': 'auto', 'ep_num_redundant_experts': 0, 'ep_dispatch_algorithm': None, 'init_expert_location': 'trivial', 'enable_eplb': False, 'enable_async_eplb': False, 'eplb_algorithm': 'auto', 'eplb_rebalance_num_iterations': 1000, 'eplb_rebalance_layers_per_chunk': None, 'eplb_min_rebalancing_utilization_threshold': 1.0, 'expert_distribution_recorder_mode': None, 'expert_distribution_recorder_buffer_size': 1000, 'enable_expert_distribution_metrics': False, 'deepep_config': None, 'moe_dense_tp_size': None, 'elastic_ep_backend': None, 'mooncake_ib_device': None, 'max_mamba_cache_size': None, 'mamba_ssm_dtype': 'float32', 'mamba_full_memory_ratio': 0.9, 'mamba_scheduler_strategy': 'no_buffer', 'mamba_track_interval': 256, 'enable_hierarchical_cache': False, 'hicache_ratio': 2.0, 'hicache_size': 0, 'hicache_write_policy': 'write_through', 'hicache_io_backend': 'kernel', 'hicache_mem_layout': 'layer_first', 'hicache_storage_backend': None, 'hicache_storage_prefetch_policy': 'best_effort', 'hicache_storage_backend_extra_config': None, 'enable_lmcache': False, 'kt_weight_path': None, 'kt_method': 'AMXINT4', 'kt_cpuinfer': None, 'kt_threadpool_count': 2, 'kt_num_gpu_experts': None, 'kt_max_deferred_experts_per_token': None, 'dllm_algorithm': None, 'dllm_algorithm_config': None, 'enable_double_sparsity': False, 'ds_channel_config_path': None, 'ds_heavy_channel_num': 32, 'ds_heavy_token_num': 256, 'ds_heavy_channel_type': 'qk', 'ds_sparse_decode_threshold': 4096, 'cpu_offload_gb': 0, 'offload_group_size': -1, 'offload_num_in_group': 1, 'offload_prefetch_step': 1, 'offload_mode': 'cpu', 'multi_item_scoring_delimiter': None, 'disable_radix_cache': False, 'cuda_graph_max_bs': 64, 'cuda_graph_bs': [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], 'disable_cuda_graph': True, 'disable_cuda_graph_padding': False, 'enable_profile_cuda_graph': False, 'enable_cudagraph_gc': False, 'enable_layerwise_nvtx_marker': False, 'enable_nccl_nvls': False, 'enable_symm_mem': False, 'disable_flashinfer_cutlass_moe_fp4_allgather': False, 'enable_tokenizer_batch_encode': False, 'disable_tokenizer_batch_decode': False, 'disable_outlines_disk_cache': False, 'disable_custom_all_reduce': True, 'enable_mscclpp': False, 'enable_torch_symm_mem': False, 'disable_overlap_schedule': False, 'enable_mixed_chunk': False, 'enable_dp_attention': False, 'enable_dp_lm_head': False, 'enable_two_batch_overlap': False, 'enable_single_batch_overlap': False, 'tbo_token_distribution_threshold': 0.48, 'enable_torch_compile': False, 'enable_piecewise_cuda_graph': False, 'enable_torch_compile_debug_mode': False, 'torch_compile_max_bs': 32, 'piecewise_cuda_graph_max_tokens': 4096, 'piecewise_cuda_graph_tokens': [4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], 'piecewise_cuda_graph_compiler': 'eager', 'torchao_config': '', 'enable_nan_detection': False, 'enable_p2p_check': False, 'triton_attention_reduce_in_fp32': False, 'triton_attention_num_kv_splits': 8, 'triton_attention_split_tile_size': None, 'num_continuous_decode_steps': 1, 'delete_ckpt_after_loading': False, 'enable_memory_saver': False, 'enable_weights_cpu_backup': False, 'enable_draft_weights_cpu_backup': False, 'allow_auto_truncate': False, 'enable_custom_logit_processor': False, 'flashinfer_mla_disable_ragged': False, 'disable_shared_experts_fusion': False, 'disable_chunked_prefix_cache': False, 'disable_fast_image_processor': False, 'keep_mm_feature_on_device': False, 'enable_return_hidden_states': False, 'enable_return_routed_experts': False, 'scheduler_recv_interval': 1, 'numa_node': None, 'enable_deterministic_inference': False, 'rl_on_policy_target': None, 'enable_attn_tp_input_scattered': False, 'enable_nsa_prefill_context_parallel': False, 'enable_fused_qk_norm_rope': False, 'enable_dynamic_batch_tokenizer': False, 'dynamic_batch_tokenizer_batch_size': 32, 'dynamic_batch_tokenizer_batch_timeout': 0.002, 'debug_tensor_dump_output_folder': './', 'debug_tensor_dump_layers': None, 'debug_tensor_dump_input_file': None, 'debug_tensor_dump_inject': False, 'disaggregation_mode': 'null', 'disaggregation_transfer_backend': 'mooncake', 'disaggregation_bootstrap_port': 8998, 'disaggregation_decode_tp': None, 'disaggregation_decode_dp': None, 'disaggregation_prefill_pp': 1, 'disaggregation_ib_device': None, 'disaggregation_decode_enable_offload_kvcache': False, 'num_reserved_decode_tokens': 512, 'disaggregation_decode_polling_interval': 1, 'encoder_only': False, 'language_only': False, 'encoder_transfer_backend': 'zmq_to_scheduler', 'encoder_urls': [], 'custom_weight_loader': [], 'weight_loader_disable_mmap': False, 'remote_instance_weight_loader_seed_instance_ip': None, 'remote_instance_weight_loader_seed_instance_service_port': None, 'remote_instance_weight_loader_send_weights_group_ports': None, 'remote_instance_weight_loader_backend': 'nccl', 'remote_instance_weight_loader_start_seed_via_transfer_engine': False, 'enable_pdmux': False, 'pdmux_config_path': None, 'sm_group_num': 8, 'mm_max_concurrent_calls': 32, 'mm_per_request_timeout': 10.0, 'enable_broadcast_mm_inputs_process': False, 'enable_prefix_mm_cache': False, 'mm_enable_dp_encoder': False, 'mm_process_config': {}, 'decrypted_config_file': None, 'decrypted_draft_config_file': None, 'forward_hooks': None, 'status': 'ready', 'max_total_num_tokens': 1497216, 'max_req_input_len': 131066, 'internal_states': [{'model_path': '/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct', 'tokenizer_path': '/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct', 'tokenizer_mode': 'auto', 'tokenizer_worker_num': 1, 'skip_tokenizer_init': False, 'load_format': 'auto', 'model_loader_extra_config': '{}', 'trust_remote_code': False, 'context_length': None, 'is_embedding': False, 'enable_multimodal': None, 'limit_mm_data_per_request': None, 'revision': None, 'model_impl': 'auto', 'host': '127.0.0.1', 'port': 21000, 'fastapi_root_path': '', 'grpc_mode': False, 'skip_server_warmup': True, 'warmups': None, 'nccl_port': None, 'checkpoint_engine_wait_weights_before_ready': False, 'dtype': 'auto', 'quantization': None, 'quantization_param_path': None, 'kv_cache_dtype': 'auto', 'enable_fp32_lm_head': False, 'modelopt_quant': None, 'modelopt_checkpoint_restore_path': None, 'modelopt_checkpoint_save_path': None, 'modelopt_export_path': None, 'quantize_and_serve': False, 'rl_quant_profile': None, 'mem_fraction_static': 0.792, 'max_running_requests': None, 'max_queued_requests': None, 'max_total_tokens': None, 'chunked_prefill_size': 8192, 'enable_dynamic_chunking': False, 'max_prefill_tokens': 16384, 'prefill_max_requests': None, 'schedule_policy': 'fcfs', 'enable_priority_scheduling': False, 'abort_on_priority_when_disabled': False, 'schedule_low_priority_values_first': False, 'priority_scheduling_preemption_threshold': 10, 'schedule_conservativeness': 1.0, 'page_size': 128, 'hybrid_kvcache_ratio': None, 'swa_full_tokens_ratio': 0.8, 'disable_hybrid_swa_memory': False, 'radix_eviction_policy': 'lru', 'device': 'npu', 'tp_size': 1, 'pp_size': 1, 'pp_max_micro_batch_size': 4096, 'pp_async_batch_depth': 0, 'stream_interval': 1, 'stream_output': False, 'random_seed': 443814053, 'constrained_json_whitespace_pattern': None, 'constrained_json_disable_any_whitespace': False, 'watchdog_timeout': 300, 'soft_watchdog_timeout': None, 'dist_timeout': None, 'download_dir': None, 'base_gpu_id': 0, 'gpu_id_step': 1, 'sleep_on_idle': False, 'custom_sigquit_handler': None, 'log_level': 'info', 'log_level_http': None, 'log_requests': False, 'log_requests_level': 2, 'crash_dump_folder': None, 'show_time_cost': False, 'enable_metrics': False, 'enable_metrics_for_all_schedulers': False, 'tokenizer_metrics_custom_labels_header': 'x-custom-labels', 'tokenizer_metrics_allowed_custom_labels': None, 'bucket_time_to_first_token': None, 'bucket_inter_token_latency': None, 'bucket_e2e_request_latency': None, 'collect_tokens_histogram': False, 'prompt_tokens_buckets': None, 'generation_tokens_buckets': None, 'gc_warning_threshold_secs': 0.0, 'decode_log_interval': 40, 'enable_request_time_stats_logging': False, 'kv_events_config': None, 'enable_trace': False, 'otlp_traces_endpoint': 'localhost:4317', 'export_metrics_to_file': False, 'export_metrics_to_file_dir': None, 'api_key': None, 'served_model_name': '/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct', 'weight_version': 'default', 'chat_template': None, 'completion_template': None, 'file_storage_path': 'sglang_storage', 'enable_cache_report': False, 'reasoning_parser': None, 'tool_call_parser': None, 'tool_server': None, 'sampling_defaults': 'model', 'dp_size': 1, 'load_balance_method': 'round_robin', 'load_watch_interval': 0.1, 'prefill_round_robin_balance': False, 'dist_init_addr': None, 'nnodes': 1, 'node_rank': 0, 'json_model_override_args': '{}', 'preferred_sampling_params': None, 'enable_lora': None, 'max_lora_rank': None, 'lora_target_modules': None, 'lora_paths': None, 'max_loaded_loras': None, 'max_loras_per_batch': 8, 'lora_eviction_policy': 'lru', 'lora_backend': 'csgmv', 'max_lora_chunk_size': 16, 'attention_backend': 'ascend', 'decode_attention_backend': 'ascend', 'prefill_attention_backend': 'ascend', 'sampling_backend': 'pytorch', 'grammar_backend': 'xgrammar', 'mm_attention_backend': None, 'fp8_gemm_runner_backend': 'auto', 'nsa_prefill_backend': 'flashmla_sparse', 'nsa_decode_backend': 'fa3', 'disable_flashinfer_autotune': False, 'speculative_algorithm': None, 'speculative_draft_model_path': None, 'speculative_draft_model_revision': None, 'speculative_draft_load_format': None, 'speculative_num_steps': None, 'speculative_eagle_topk': None, 'speculative_num_draft_tokens': None, 'speculative_accept_threshold_single': 1.0, 'speculative_accept_threshold_acc': 1.0, 'speculative_token_map': None, 'speculative_attention_mode': 'prefill', 'speculative_draft_attention_backend': None, 'speculative_moe_runner_backend': 'auto', 'speculative_moe_a2a_backend': None, 'speculative_draft_model_quantization': None, 'speculative_ngram_min_match_window_size': 1, 'speculative_ngram_max_match_window_size': 12, 'speculative_ngram_min_bfs_breadth': 1, 'speculative_ngram_max_bfs_breadth': 10, 'speculative_ngram_match_type': 'BFS', 'speculative_ngram_branch_length': 18, 'speculative_ngram_capacity': 10000000, 'enable_mtp': False, 'ep_size': 1, 'moe_a2a_backend': 'none', 'moe_runner_backend': 'auto', 'flashinfer_mxfp4_moe_precision': 'default', 'enable_flashinfer_allreduce_fusion': False, 'deepep_mode': 'auto', 'ep_num_redundant_experts': 0, 'ep_dispatch_algorithm': None, 'init_expert_location': 'trivial', 'enable_eplb': False, 'enable_async_eplb': False, 'eplb_algorithm': 'auto', 'eplb_rebalance_num_iterations': 1000, 'eplb_rebalance_layers_per_chunk': None, 'eplb_min_rebalancing_utilization_threshold': 1.0, 'expert_distribution_recorder_mode': None, 'expert_distribution_recorder_buffer_size': 1000, 'enable_expert_distribution_metrics': False, 'deepep_config': None, 'moe_dense_tp_size': None, 'elastic_ep_backend': None, 'mooncake_ib_device': None, 'max_mamba_cache_size': None, 'mamba_ssm_dtype': 'float32', 'mamba_full_memory_ratio': 0.9, 'mamba_scheduler_strategy': 'no_buffer', 'mamba_track_interval': 256, 'enable_hierarchical_cache': False, 'hicache_ratio': 2.0, 'hicache_size': 0, 'hicache_write_policy': 'write_through', 'hicache_io_backend': 'kernel', 'hicache_mem_layout': 'layer_first', 'hicache_storage_backend': None, 'hicache_storage_prefetch_policy': 'best_effort', 'hicache_storage_backend_extra_config': None, 'enable_lmcache': False, 'kt_weight_path': None, 'kt_method': 'AMXINT4', 'kt_cpuinfer': None, 'kt_threadpool_count': 2, 'kt_num_gpu_experts': None, 'kt_max_deferred_experts_per_token': None, 'dllm_algorithm': None, 'dllm_algorithm_config': None, 'enable_double_sparsity': False, 'ds_channel_config_path': None, 'ds_heavy_channel_num': 32, 'ds_heavy_token_num': 256, 'ds_heavy_channel_type': 'qk', 'ds_sparse_decode_threshold': 4096, 'cpu_offload_gb': 0, 'offload_group_size': -1, 'offload_num_in_group': 1, 'offload_prefetch_step': 1, 'offload_mode': 'cpu', 'multi_item_scoring_delimiter': None, 'disable_radix_cache': False, 'cuda_graph_max_bs': 64, 'cuda_graph_bs': [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], 'disable_cuda_graph': True, 'disable_cuda_graph_padding': False, 'enable_profile_cuda_graph': False, 'enable_cudagraph_gc': False, 'enable_layerwise_nvtx_marker': False, 'enable_nccl_nvls': False, 'enable_symm_mem': False, 'disable_flashinfer_cutlass_moe_fp4_allgather': False, 'enable_tokenizer_batch_encode': False, 'disable_tokenizer_batch_decode': False, 'disable_outlines_disk_cache': False, 'disable_custom_all_reduce': True, 'enable_mscclpp': False, 'enable_torch_symm_mem': False, 'disable_overlap_schedule': False, 'enable_mixed_chunk': False, 'enable_dp_attention': False, 'enable_dp_lm_head': False, 'enable_two_batch_overlap': False, 'enable_single_batch_overlap': False, 'tbo_token_distribution_threshold': 0.48, 'enable_torch_compile': False, 'enable_piecewise_cuda_graph': False, 'enable_torch_compile_debug_mode': False, 'torch_compile_max_bs': 32, 'piecewise_cuda_graph_max_tokens': 4096, 'piecewise_cuda_graph_tokens': [4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], 'piecewise_cuda_graph_compiler': 'eager', 'torchao_config': '', 'enable_nan_detection': False, 'enable_p2p_check': False, 'triton_attention_reduce_in_fp32': False, 'triton_attention_num_kv_splits': 8, 'triton_attention_split_tile_size': None, 'num_continuous_decode_steps': 1, 'delete_ckpt_after_loading': False, 'enable_memory_saver': False, 'enable_weights_cpu_backup': False, 'enable_draft_weights_cpu_backup': False, 'allow_auto_truncate': False, 'enable_custom_logit_processor': False, 'flashinfer_mla_disable_ragged': False, 'disable_shared_experts_fusion': False, 'disable_chunked_prefix_cache': True, 'disable_fast_image_processor': False, 'keep_mm_feature_on_device': False, 'enable_return_hidden_states': False, 'enable_return_routed_experts': False, 'scheduler_recv_interval': 1, 'numa_node': None, 'enable_deterministic_inference': False, 'rl_on_policy_target': None, 'enable_attn_tp_input_scattered': False, 'enable_nsa_prefill_context_parallel': False, 'enable_fused_qk_norm_rope': False, 'enable_dynamic_batch_tokenizer': False, 'dynamic_batch_tokenizer_batch_size': 32, 'dynamic_batch_tokenizer_batch_timeout': 0.002, 'debug_tensor_dump_output_folder': './', 'debug_tensor_dump_layers': None, 'debug_tensor_dump_input_file': None, 'debug_tensor_dump_inject': False, 'disaggregation_mode': 'null', 'disaggregation_transfer_backend': 'mooncake', 'disaggregation_bootstrap_port': 8998, 'disaggregation_decode_tp': None, 'disaggregation_decode_dp': None, 'disaggregation_prefill_pp': 1, 'disaggregation_ib_device': None, 'disaggregation_decode_enable_offload_kvcache': False, 'num_reserved_decode_tokens': 512, 'disaggregation_decode_polling_interval': 1, 'encoder_only': False, 'language_only': False, 'encoder_transfer_backend': 'zmq_to_scheduler', 'encoder_urls': [], 'custom_weight_loader': [], 'weight_loader_disable_mmap': False, 'remote_instance_weight_loader_seed_instance_ip': None, 'remote_instance_weight_loader_seed_instance_service_port': None, 'remote_instance_weight_loader_send_weights_group_ports': None, 'remote_instance_weight_loader_backend': 'nccl', 'remote_instance_weight_loader_start_seed_via_transfer_engine': False, 'enable_pdmux': False, 'pdmux_config_path': None, 'sm_group_num': 8, 'mm_max_concurrent_calls': 32, 'mm_per_request_timeout': 10.0, 'enable_broadcast_mm_inputs_process': False, 'enable_prefix_mm_cache': False, 'mm_enable_dp_encoder': False, 'mm_process_config': {}, 'decrypted_config_file': None, 'decrypted_draft_config_file': None, 'forward_hooks': None, 'use_mla_backend': False, 'last_gen_throughput': 0.5018596586556519, 'memory_usage': {'weight': 2.46, 'kvcache': 45.7, 'token_capacity': 1497216, 'graph': 0}}], 'version': '0.5.6.post2'}, 'duration': 601.895416218962, 'completed': 128, 'total_input_tokens': 481256, 'total_input_text_tokens': 481256, 'total_input_vision_tokens': 0, 'total_output_tokens': 128, 'total_output_tokens_retokenized': 127, 'request_throughput': 0.21266152981207484, 'input_throughput': 799.5674780565616, 'output_throughput': 0.21266152981207484, 'total_throughput': 799.7801395863737, 'mean_e2e_latency_ms': np.float64(73687.84457176525), 'median_e2e_latency_ms': np.float64(75114.97771547874), 'std_e2e_latency_ms': np.float64(10818.807017366176), 'p99_e2e_latency_ms': np.float64(98016.12273537321), 'mean_ttft_ms': np.float64(73126.0301676648), 'median_ttft_ms': np.float64(75114.96635549702), 'std_ttft_ms': np.float64(12614.57003898559), 'p99_ttft_ms': np.float64(98016.11463517068), 'mean_tpot_ms': np.float64(0.0), 'median_tpot_ms': np.float64(0.0), 'std_tpot_ms': np.float64(0.0), 'p99_tpot_ms': np.float64(0.0), 'mean_itl_ms': np.float64(0.0), 'median_itl_ms': np.float64(0.0), 'std_itl_ms': np.float64(0.0), 'p95_itl_ms': np.float64(0.0), 'p99_itl_ms': np.float64(0.0), 'concurrency': np.float64(15.670569755185994), 'accept_length': None, 'max_output_tokens_per_s': 8.0, 'max_concurrent_requests': 20, 'input_lens': [3764, 3751, 3742, 3748, 3757, 3753, 3767, 3757, 3761, 3757, 3751, 3774, 3756, 3755, 3742, 3754, 3785, 3749, 3762, 3766, 3759, 3764, 3774, 3773, 3789, 3744, 3748, 3758, 3733, 3759, 3768, 3749, 3746, 3768, 3748, 3763, 3751, 3757, 3788, 3742, 3747, 3768, 3760, 3752, 3763, 3751, 3755, 3754, 3780, 3731, 3766, 3748, 3775, 3762, 3763, 3757, 3774, 3765, 3787, 3757, 3767, 3766, 3755, 3757, 3749, 3740, 3759, 3765, 3775, 3770, 3764, 3757, 3767, 3756, 3760, 3783, 3720, 3764, 3763, 3746, 3766, 3751, 3767, 3775, 3771, 3752, 3768, 3770, 3753, 3764, 3755, 3763, 3781, 3767, 3742, 3752, 3742, 3757, 3784, 3765, 3768, 3767, 3768, 3742, 3754, 3762, 3781, 3769, 3760, 3751, 3747, 3734, 3749, 3756, 3761, 3769, 3752, 3731, 3762, 3765, 3755, 3778, 3770, 3750, 3758, 3777, 3776, 3769], 'output_lens': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'ttfts': [20.575704680988565, 39.412799782003276, 39.4121941489866, 39.41161817603279, 39.410684560018126, 55.779396861966234, 55.77881496900227, 55.778468387026805, 55.777500791009516, 76.00795606704196, 76.00781252601882, 76.00744376401417, 76.00664284895174, 91.95224000298185, 91.95173354900908, 91.95098615600727, 71.38112941000145, 68.04372678295476, 68.04337037104415, 68.04318933998002, 68.04276924696751, 71.91228604898788, 71.91142649302492, 0.0, 71.91032495599939, 74.50311845500255, 74.50227523001377, 74.50149314495502, 74.50170904694824, 77.59924764599418, 77.59800800896483, 77.59793651901418, 77.59771372703835, 82.9726848899736, 82.97209702600958, 82.97130755096441, 99.00671853299718, 78.77112069597933, 78.77081105404068, 78.77051705203485, 96.81489533401327, 73.99581324501196, 73.9950542699662, 73.9945731680491, 91.61907707503997, 72.57923099200707, 72.57872370904079, 72.57830498600379, 98.46040122600971, 77.58741022797767, 77.58691464498406, 77.58598804002395, 80.61215413198806, 80.61142428795574, 80.61058242298895, 80.61068035295466, 80.98290013003862, 80.98274221899919, 80.98244002700085, 80.98197962500853, 81.30510134797078, 81.3040355219855, 81.30393708101474, 81.30353499902412, 73.42448264698032, 73.42324212903623, 73.42326811002567, 73.42304614803288, 71.78022036200855, 71.7794257280184, 71.77922461699927, 71.77832648099866, 72.64182257500943, 72.6412883020239, 72.64082325797062, 72.63992241298547, 73.38274477300001, 73.3822378400364, 73.38197427801788, 73.38120599399554, 74.28999902697979, 74.28924755298067, 74.28878363897093, 74.28814942599274, 76.16146203799872, 76.16058409196557, 76.15979324700311, 76.15988305700012, 75.59381206397666, 75.59296340902802, 75.59225046500796, 75.59233523503644, 77.23067923495546, 77.23021864297334, 77.2297563199536, 77.22914984601084, 76.83791165804723, 76.83721925399732, 76.83700452302583, 76.8363090280327, 76.36945427500177, 76.36841897899285, 76.36848316999385, 76.36810459702974, 76.30470576800872, 76.30424987501465, 76.30352820100961, 76.30372878198978, 73.8768201530329, 73.87635367101757, 73.87589402799495, 73.87529472395545, 74.63768224598607, 74.63753046497004, 74.6372743230313, 74.63683848094661, 75.93413948797388, 75.93368087499402, 75.9334180339938, 75.93264522898244, 71.65752573404461, 71.65732967300573, 71.65716096200049, 71.65680869901553, 53.89307390496833, 53.89213944901712, 53.89207439799793, 53.89185829804046], 'itls': [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []], 'generated_texts': ['.', '�', '�', 'ure', '�', 'ch', '�', '�', '.', '�', '�', '�', '\r\n', '�', '�', '�', 's', '�', '.', 'у', 'unic', 'д', 's', '', '�', '�', '�', 'z', '�', '�', '�', '�', '�', 'es', '.', '```', '�', '�', '�', '�', ' F', '�', '/', '�', '�', '�', 'it', '�', '�', '�', '�', 'er', '�', '_', '�', '�', '�', '�', '�', '�', 'z', '�', '�', ')', '�', '�', '�', 'side', '�', 'ing', 'change', '�', '�', 's', '�', '�', '�', '�', 'it', 're', '�', '�', '�', '_', '�', '�', 's', 'ur', '�', '�', '�', '�', 'ال', '�', '�', 'es', '�', 'ider', 'в', 'I', '�', '�', '�', '�', '�', 'h', '�', 'ng', '�', '�', '�', '�', ' B', '�', '�', ' F', 't', '�', '_', 'tern', '�', '.', ' B', '�', '�', '�', 'I', '�'], 'errors': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']}
command=python3 -m sglang.launch_server --model-path /data/ascend-ci-share-pkking-sglang/modelscope/hub/models/Qwen/Qwen3-32B --trust-remote-code --tp-size 4 --mem-fraction-static 0.8 --max-running-requests 16 --chunked-prefill-size -1 --disable-cuda-graph --base-gpu-id 8 --enable-hierarchical-cache --hicache-ratio 5 --hicache-write-policy write_through --attention-backend ascend --device npu --host 127.0.0.1 --port 21000
benchmark_args=namespace(backend='sglang', base_url='http://127.0.0.1:21000', host=None, port=None, dataset_name='generated-shared-prefix', dataset_path='', model=None, tokenizer=None, num_prompts=128, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=3584, random_output_len=1, random_range_ratio=0.0, request_rate=inf, multi=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, return_routed_experts=False, seed=0, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=None, lora_name=None, lora_request_distribution='uniform', lora_zipf_alpha=1.5, prompt_suffix='', device='npu', pd_separated=False, gsp_num_groups=1, gsp_prompts_per_group=128, gsp_system_prompt_len=1792, gsp_question_len=1792, gsp_output_len=1, max_concurrency=16, warmup_requests=1, output_details=False, tokenize_prompt=False, plot_throughput=False, use_trace_timestamps=False, mooncake_slowdown_factor=1.0, mooncake_num_rounds=1, served_model_name=None)
namespace(backend='sglang', base_url='http://127.0.0.1:21000', host=None, port=30000, dataset_name='generated-shared-prefix', dataset_path='', model='/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct', tokenizer=None, num_prompts=128, sharegpt_output_len=None, sharegpt_context_len=None, random_input_len=3584, random_output_len=1, random_range_ratio=0.0, request_rate=inf, multi=None, output_file=None, disable_tqdm=False, disable_stream=False, return_logprob=False, return_routed_experts=False, seed=0, disable_ignore_eos=False, extra_request_body=None, apply_chat_template=False, profile=None, lora_name=None, lora_request_distribution='uniform', lora_zipf_alpha=1.5, prompt_suffix='', device='npu', pd_separated=False, gsp_num_groups=1, gsp_prompts_per_group=128, gsp_system_prompt_len=1792, gsp_question_len=1792, gsp_output_len=1, max_concurrency=16, warmup_requests=1, output_details=False, tokenize_prompt=False, plot_throughput=False, use_trace_timestamps=False, mooncake_slowdown_factor=1.0, mooncake_num_rounds=1, served_model_name=None)


Loading cached generated input data from /root/.cache/sglang/benchmark/gen_shared_prefix_0_1_128_1792_1792_1_PreTrainedTokenizerFast.pkl
Starting warmup with 1 sequences...
Warmup completed with 1 sequences. Starting main benchmark run...

============ Serving Benchmark Result ============
Backend:                                 sglang    
Traffic request rate:                    inf       
Max request concurrency:                 16        
Successful requests:                     128       
Benchmark duration (s):                  22.61     
Total input tokens:                      481256    
Total input text tokens:                 481256    
Total input vision tokens:               0         
Total generated tokens:                  128       
Total generated tokens (retokenized):    127       
Request throughput (req/s):              5.66      
Input token throughput (tok/s):          21286.96  
Output token throughput (tok/s):         5.66      
Peak output token throughput (tok/s):    17.00     
Peak concurrent requests:                32        
Total token throughput (tok/s):          21292.62  
Concurrency:                             15.90     
----------------End-to-End Latency----------------
Mean E2E Latency (ms):                   2808.30   
Median E2E Latency (ms):                 2510.65   
---------------Time to First Token----------------
Mean TTFT (ms):                          2788.66   
Median TTFT (ms):                        2455.64   
P99 TTFT (ms):                           4556.36   
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          0.00      
Median TPOT (ms):                        0.00      
P99 TPOT (ms):                           0.00      
---------------Inter-Token Latency----------------
Mean ITL (ms):                           0.00      
Median ITL (ms):                         0.00      
P95 ITL (ms):                            0.00      
P99 ITL (ms):                            0.00      
Max ITL (ms):                            0.00      
==================================================
--------------------------------------------- {'tag': None, 'backend': 'sglang', 'dataset_name': 'generated-shared-prefix', 'request_rate': inf, 'max_concurrency': 16, 'sharegpt_output_len': None, 'random_input_len': 3584, 'random_output_len': 1, 'random_range_ratio': 0.0, 'server_info': {'model_path': '/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct', 'tokenizer_path': '/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct', 'tokenizer_mode': 'auto', 'tokenizer_worker_num': 1, 'skip_tokenizer_init': False, 'load_format': 'auto', 'model_loader_extra_config': '{}', 'trust_remote_code': False, 'context_length': None, 'is_embedding': False, 'enable_multimodal': None, 'limit_mm_data_per_request': None, 'revision': None, 'model_impl': 'auto', 'host': '127.0.0.1', 'port': 21000, 'fastapi_root_path': '', 'grpc_mode': False, 'skip_server_warmup': True, 'warmups': None, 'nccl_port': None, 'checkpoint_engine_wait_weights_before_ready': False, 'dtype': 'auto', 'quantization': None, 'quantization_param_path': None, 'kv_cache_dtype': 'auto', 'enable_fp32_lm_head': False, 'modelopt_quant': None, 'modelopt_checkpoint_restore_path': None, 'modelopt_checkpoint_save_path': None, 'modelopt_export_path': None, 'quantize_and_serve': False, 'rl_quant_profile': None, 'mem_fraction_static': 0.792, 'max_running_requests': None, 'max_queued_requests': None, 'max_total_tokens': None, 'chunked_prefill_size': 8192, 'enable_dynamic_chunking': False, 'max_prefill_tokens': 16384, 'prefill_max_requests': None, 'schedule_policy': 'fcfs', 'enable_priority_scheduling': False, 'abort_on_priority_when_disabled': False, 'schedule_low_priority_values_first': False, 'priority_scheduling_preemption_threshold': 10, 'schedule_conservativeness': 1.0, 'page_size': 128, 'hybrid_kvcache_ratio': None, 'swa_full_tokens_ratio': 0.8, 'disable_hybrid_swa_memory': False, 'radix_eviction_policy': 'lru', 'device': 'npu', 'tp_size': 1, 'pp_size': 1, 'pp_max_micro_batch_size': None, 'pp_async_batch_depth': 0, 'stream_interval': 1, 'stream_output': False, 'random_seed': 443814053, 'constrained_json_whitespace_pattern': None, 'constrained_json_disable_any_whitespace': False, 'watchdog_timeout': 300, 'soft_watchdog_timeout': None, 'dist_timeout': None, 'download_dir': None, 'base_gpu_id': 0, 'gpu_id_step': 1, 'sleep_on_idle': False, 'custom_sigquit_handler': None, 'log_level': 'info', 'log_level_http': None, 'log_requests': False, 'log_requests_level': 2, 'crash_dump_folder': None, 'show_time_cost': False, 'enable_metrics': False, 'enable_metrics_for_all_schedulers': False, 'tokenizer_metrics_custom_labels_header': 'x-custom-labels', 'tokenizer_metrics_allowed_custom_labels': None, 'bucket_time_to_first_token': None, 'bucket_inter_token_latency': None, 'bucket_e2e_request_latency': None, 'collect_tokens_histogram': False, 'prompt_tokens_buckets': None, 'generation_tokens_buckets': None, 'gc_warning_threshold_secs': 0.0, 'decode_log_interval': 40, 'enable_request_time_stats_logging': False, 'kv_events_config': None, 'enable_trace': False, 'otlp_traces_endpoint': 'localhost:4317', 'export_metrics_to_file': False, 'export_metrics_to_file_dir': None, 'api_key': None, 'served_model_name': '/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct', 'weight_version': 'default', 'chat_template': None, 'completion_template': None, 'file_storage_path': 'sglang_storage', 'enable_cache_report': False, 'reasoning_parser': None, 'tool_call_parser': None, 'tool_server': None, 'sampling_defaults': 'model', 'dp_size': 1, 'load_balance_method': 'round_robin', 'load_watch_interval': 0.1, 'prefill_round_robin_balance': False, 'dist_init_addr': None, 'nnodes': 1, 'node_rank': 0, 'json_model_override_args': '{}', 'preferred_sampling_params': None, 'enable_lora': None, 'max_lora_rank': None, 'lora_target_modules': None, 'lora_paths': None, 'max_loaded_loras': None, 'max_loras_per_batch': 8, 'lora_eviction_policy': 'lru', 'lora_backend': 'csgmv', 'max_lora_chunk_size': 16, 'attention_backend': 'ascend', 'decode_attention_backend': 'ascend', 'prefill_attention_backend': 'ascend', 'sampling_backend': 'pytorch', 'grammar_backend': 'xgrammar', 'mm_attention_backend': None, 'fp8_gemm_runner_backend': 'auto', 'nsa_prefill_backend': 'flashmla_sparse', 'nsa_decode_backend': 'fa3', 'disable_flashinfer_autotune': False, 'speculative_algorithm': None, 'speculative_draft_model_path': None, 'speculative_draft_model_revision': None, 'speculative_draft_load_format': None, 'speculative_num_steps': None, 'speculative_eagle_topk': None, 'speculative_num_draft_tokens': None, 'speculative_accept_threshold_single': 1.0, 'speculative_accept_threshold_acc': 1.0, 'speculative_token_map': None, 'speculative_attention_mode': 'prefill', 'speculative_draft_attention_backend': None, 'speculative_moe_runner_backend': 'auto', 'speculative_moe_a2a_backend': None, 'speculative_draft_model_quantization': None, 'speculative_ngram_min_match_window_size': 1, 'speculative_ngram_max_match_window_size': 12, 'speculative_ngram_min_bfs_breadth': 1, 'speculative_ngram_max_bfs_breadth': 10, 'speculative_ngram_match_type': 'BFS', 'speculative_ngram_branch_length': 18, 'speculative_ngram_capacity': 10000000, 'enable_mtp': False, 'ep_size': 1, 'moe_a2a_backend': 'none', 'moe_runner_backend': 'auto', 'flashinfer_mxfp4_moe_precision': 'default', 'enable_flashinfer_allreduce_fusion': False, 'deepep_mode': 'auto', 'ep_num_redundant_experts': 0, 'ep_dispatch_algorithm': None, 'init_expert_location': 'trivial', 'enable_eplb': False, 'enable_async_eplb': False, 'eplb_algorithm': 'auto', 'eplb_rebalance_num_iterations': 1000, 'eplb_rebalance_layers_per_chunk': None, 'eplb_min_rebalancing_utilization_threshold': 1.0, 'expert_distribution_recorder_mode': None, 'expert_distribution_recorder_buffer_size': 1000, 'enable_expert_distribution_metrics': False, 'deepep_config': None, 'moe_dense_tp_size': None, 'elastic_ep_backend': None, 'mooncake_ib_device': None, 'max_mamba_cache_size': None, 'mamba_ssm_dtype': 'float32', 'mamba_full_memory_ratio': 0.9, 'mamba_scheduler_strategy': 'no_buffer', 'mamba_track_interval': 256, 'enable_hierarchical_cache': False, 'hicache_ratio': 2.0, 'hicache_size': 0, 'hicache_write_policy': 'write_through', 'hicache_io_backend': 'kernel', 'hicache_mem_layout': 'layer_first', 'hicache_storage_backend': None, 'hicache_storage_prefetch_policy': 'best_effort', 'hicache_storage_backend_extra_config': None, 'enable_lmcache': False, 'kt_weight_path': None, 'kt_method': 'AMXINT4', 'kt_cpuinfer': None, 'kt_threadpool_count': 2, 'kt_num_gpu_experts': None, 'kt_max_deferred_experts_per_token': None, 'dllm_algorithm': None, 'dllm_algorithm_config': None, 'enable_double_sparsity': False, 'ds_channel_config_path': None, 'ds_heavy_channel_num': 32, 'ds_heavy_token_num': 256, 'ds_heavy_channel_type': 'qk', 'ds_sparse_decode_threshold': 4096, 'cpu_offload_gb': 0, 'offload_group_size': -1, 'offload_num_in_group': 1, 'offload_prefetch_step': 1, 'offload_mode': 'cpu', 'multi_item_scoring_delimiter': None, 'disable_radix_cache': False, 'cuda_graph_max_bs': 64, 'cuda_graph_bs': [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], 'disable_cuda_graph': True, 'disable_cuda_graph_padding': False, 'enable_profile_cuda_graph': False, 'enable_cudagraph_gc': False, 'enable_layerwise_nvtx_marker': False, 'enable_nccl_nvls': False, 'enable_symm_mem': False, 'disable_flashinfer_cutlass_moe_fp4_allgather': False, 'enable_tokenizer_batch_encode': False, 'disable_tokenizer_batch_decode': False, 'disable_outlines_disk_cache': False, 'disable_custom_all_reduce': True, 'enable_mscclpp': False, 'enable_torch_symm_mem': False, 'disable_overlap_schedule': False, 'enable_mixed_chunk': False, 'enable_dp_attention': False, 'enable_dp_lm_head': False, 'enable_two_batch_overlap': False, 'enable_single_batch_overlap': False, 'tbo_token_distribution_threshold': 0.48, 'enable_torch_compile': False, 'enable_piecewise_cuda_graph': False, 'enable_torch_compile_debug_mode': False, 'torch_compile_max_bs': 32, 'piecewise_cuda_graph_max_tokens': 4096, 'piecewise_cuda_graph_tokens': [4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], 'piecewise_cuda_graph_compiler': 'eager', 'torchao_config': '', 'enable_nan_detection': False, 'enable_p2p_check': False, 'triton_attention_reduce_in_fp32': False, 'triton_attention_num_kv_splits': 8, 'triton_attention_split_tile_size': None, 'num_continuous_decode_steps': 1, 'delete_ckpt_after_loading': False, 'enable_memory_saver': False, 'enable_weights_cpu_backup': False, 'enable_draft_weights_cpu_backup': False, 'allow_auto_truncate': False, 'enable_custom_logit_processor': False, 'flashinfer_mla_disable_ragged': False, 'disable_shared_experts_fusion': False, 'disable_chunked_prefix_cache': False, 'disable_fast_image_processor': False, 'keep_mm_feature_on_device': False, 'enable_return_hidden_states': False, 'enable_return_routed_experts': False, 'scheduler_recv_interval': 1, 'numa_node': None, 'enable_deterministic_inference': False, 'rl_on_policy_target': None, 'enable_attn_tp_input_scattered': False, 'enable_nsa_prefill_context_parallel': False, 'enable_fused_qk_norm_rope': False, 'enable_dynamic_batch_tokenizer': False, 'dynamic_batch_tokenizer_batch_size': 32, 'dynamic_batch_tokenizer_batch_timeout': 0.002, 'debug_tensor_dump_output_folder': './', 'debug_tensor_dump_layers': None, 'debug_tensor_dump_input_file': None, 'debug_tensor_dump_inject': False, 'disaggregation_mode': 'null', 'disaggregation_transfer_backend': 'mooncake', 'disaggregation_bootstrap_port': 8998, 'disaggregation_decode_tp': None, 'disaggregation_decode_dp': None, 'disaggregation_prefill_pp': 1, 'disaggregation_ib_device': None, 'disaggregation_decode_enable_offload_kvcache': False, 'num_reserved_decode_tokens': 512, 'disaggregation_decode_polling_interval': 1, 'encoder_only': False, 'language_only': False, 'encoder_transfer_backend': 'zmq_to_scheduler', 'encoder_urls': [], 'custom_weight_loader': [], 'weight_loader_disable_mmap': False, 'remote_instance_weight_loader_seed_instance_ip': None, 'remote_instance_weight_loader_seed_instance_service_port': None, 'remote_instance_weight_loader_send_weights_group_ports': None, 'remote_instance_weight_loader_backend': 'nccl', 'remote_instance_weight_loader_start_seed_via_transfer_engine': False, 'enable_pdmux': False, 'pdmux_config_path': None, 'sm_group_num': 8, 'mm_max_concurrent_calls': 32, 'mm_per_request_timeout': 10.0, 'enable_broadcast_mm_inputs_process': False, 'enable_prefix_mm_cache': False, 'mm_enable_dp_encoder': False, 'mm_process_config': {}, 'decrypted_config_file': None, 'decrypted_draft_config_file': None, 'forward_hooks': None, 'status': 'ready', 'max_total_num_tokens': 1497216, 'max_req_input_len': 131066, 'internal_states': [{'model_path': '/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct', 'tokenizer_path': '/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct', 'tokenizer_mode': 'auto', 'tokenizer_worker_num': 1, 'skip_tokenizer_init': False, 'load_format': 'auto', 'model_loader_extra_config': '{}', 'trust_remote_code': False, 'context_length': None, 'is_embedding': False, 'enable_multimodal': None, 'limit_mm_data_per_request': None, 'revision': None, 'model_impl': 'auto', 'host': '127.0.0.1', 'port': 21000, 'fastapi_root_path': '', 'grpc_mode': False, 'skip_server_warmup': True, 'warmups': None, 'nccl_port': None, 'checkpoint_engine_wait_weights_before_ready': False, 'dtype': 'auto', 'quantization': None, 'quantization_param_path': None, 'kv_cache_dtype': 'auto', 'enable_fp32_lm_head': False, 'modelopt_quant': None, 'modelopt_checkpoint_restore_path': None, 'modelopt_checkpoint_save_path': None, 'modelopt_export_path': None, 'quantize_and_serve': False, 'rl_quant_profile': None, 'mem_fraction_static': 0.792, 'max_running_requests': None, 'max_queued_requests': None, 'max_total_tokens': None, 'chunked_prefill_size': 8192, 'enable_dynamic_chunking': False, 'max_prefill_tokens': 16384, 'prefill_max_requests': None, 'schedule_policy': 'fcfs', 'enable_priority_scheduling': False, 'abort_on_priority_when_disabled': False, 'schedule_low_priority_values_first': False, 'priority_scheduling_preemption_threshold': 10, 'schedule_conservativeness': 1.0, 'page_size': 128, 'hybrid_kvcache_ratio': None, 'swa_full_tokens_ratio': 0.8, 'disable_hybrid_swa_memory': False, 'radix_eviction_policy': 'lru', 'device': 'npu', 'tp_size': 1, 'pp_size': 1, 'pp_max_micro_batch_size': 4096, 'pp_async_batch_depth': 0, 'stream_interval': 1, 'stream_output': False, 'random_seed': 443814053, 'constrained_json_whitespace_pattern': None, 'constrained_json_disable_any_whitespace': False, 'watchdog_timeout': 300, 'soft_watchdog_timeout': None, 'dist_timeout': None, 'download_dir': None, 'base_gpu_id': 0, 'gpu_id_step': 1, 'sleep_on_idle': False, 'custom_sigquit_handler': None, 'log_level': 'info', 'log_level_http': None, 'log_requests': False, 'log_requests_level': 2, 'crash_dump_folder': None, 'show_time_cost': False, 'enable_metrics': False, 'enable_metrics_for_all_schedulers': False, 'tokenizer_metrics_custom_labels_header': 'x-custom-labels', 'tokenizer_metrics_allowed_custom_labels': None, 'bucket_time_to_first_token': None, 'bucket_inter_token_latency': None, 'bucket_e2e_request_latency': None, 'collect_tokens_histogram': False, 'prompt_tokens_buckets': None, 'generation_tokens_buckets': None, 'gc_warning_threshold_secs': 0.0, 'decode_log_interval': 40, 'enable_request_time_stats_logging': False, 'kv_events_config': None, 'enable_trace': False, 'otlp_traces_endpoint': 'localhost:4317', 'export_metrics_to_file': False, 'export_metrics_to_file_dir': None, 'api_key': None, 'served_model_name': '/root/.cache/modelscope/hub/models/LLM-Research/Llama-3.2-1B-Instruct', 'weight_version': 'default', 'chat_template': None, 'completion_template': None, 'file_storage_path': 'sglang_storage', 'enable_cache_report': False, 'reasoning_parser': None, 'tool_call_parser': None, 'tool_server': None, 'sampling_defaults': 'model', 'dp_size': 1, 'load_balance_method': 'round_robin', 'load_watch_interval': 0.1, 'prefill_round_robin_balance': False, 'dist_init_addr': None, 'nnodes': 1, 'node_rank': 0, 'json_model_override_args': '{}', 'preferred_sampling_params': None, 'enable_lora': None, 'max_lora_rank': None, 'lora_target_modules': None, 'lora_paths': None, 'max_loaded_loras': None, 'max_loras_per_batch': 8, 'lora_eviction_policy': 'lru', 'lora_backend': 'csgmv', 'max_lora_chunk_size': 16, 'attention_backend': 'ascend', 'decode_attention_backend': 'ascend', 'prefill_attention_backend': 'ascend', 'sampling_backend': 'pytorch', 'grammar_backend': 'xgrammar', 'mm_attention_backend': None, 'fp8_gemm_runner_backend': 'auto', 'nsa_prefill_backend': 'flashmla_sparse', 'nsa_decode_backend': 'fa3', 'disable_flashinfer_autotune': False, 'speculative_algorithm': None, 'speculative_draft_model_path': None, 'speculative_draft_model_revision': None, 'speculative_draft_load_format': None, 'speculative_num_steps': None, 'speculative_eagle_topk': None, 'speculative_num_draft_tokens': None, 'speculative_accept_threshold_single': 1.0, 'speculative_accept_threshold_acc': 1.0, 'speculative_token_map': None, 'speculative_attention_mode': 'prefill', 'speculative_draft_attention_backend': None, 'speculative_moe_runner_backend': 'auto', 'speculative_moe_a2a_backend': None, 'speculative_draft_model_quantization': None, 'speculative_ngram_min_match_window_size': 1, 'speculative_ngram_max_match_window_size': 12, 'speculative_ngram_min_bfs_breadth': 1, 'speculative_ngram_max_bfs_breadth': 10, 'speculative_ngram_match_type': 'BFS', 'speculative_ngram_branch_length': 18, 'speculative_ngram_capacity': 10000000, 'enable_mtp': False, 'ep_size': 1, 'moe_a2a_backend': 'none', 'moe_runner_backend': 'auto', 'flashinfer_mxfp4_moe_precision': 'default', 'enable_flashinfer_allreduce_fusion': False, 'deepep_mode': 'auto', 'ep_num_redundant_experts': 0, 'ep_dispatch_algorithm': None, 'init_expert_location': 'trivial', 'enable_eplb': False, 'enable_async_eplb': False, 'eplb_algorithm': 'auto', 'eplb_rebalance_num_iterations': 1000, 'eplb_rebalance_layers_per_chunk': None, 'eplb_min_rebalancing_utilization_threshold': 1.0, 'expert_distribution_recorder_mode': None, 'expert_distribution_recorder_buffer_size': 1000, 'enable_expert_distribution_metrics': False, 'deepep_config': None, 'moe_dense_tp_size': None, 'elastic_ep_backend': None, 'mooncake_ib_device': None, 'max_mamba_cache_size': None, 'mamba_ssm_dtype': 'float32', 'mamba_full_memory_ratio': 0.9, 'mamba_scheduler_strategy': 'no_buffer', 'mamba_track_interval': 256, 'enable_hierarchical_cache': False, 'hicache_ratio': 2.0, 'hicache_size': 0, 'hicache_write_policy': 'write_through', 'hicache_io_backend': 'kernel', 'hicache_mem_layout': 'layer_first', 'hicache_storage_backend': None, 'hicache_storage_prefetch_policy': 'best_effort', 'hicache_storage_backend_extra_config': None, 'enable_lmcache': False, 'kt_weight_path': None, 'kt_method': 'AMXINT4', 'kt_cpuinfer': None, 'kt_threadpool_count': 2, 'kt_num_gpu_experts': None, 'kt_max_deferred_experts_per_token': None, 'dllm_algorithm': None, 'dllm_algorithm_config': None, 'enable_double_sparsity': False, 'ds_channel_config_path': None, 'ds_heavy_channel_num': 32, 'ds_heavy_token_num': 256, 'ds_heavy_channel_type': 'qk', 'ds_sparse_decode_threshold': 4096, 'cpu_offload_gb': 0, 'offload_group_size': -1, 'offload_num_in_group': 1, 'offload_prefetch_step': 1, 'offload_mode': 'cpu', 'multi_item_scoring_delimiter': None, 'disable_radix_cache': False, 'cuda_graph_max_bs': 64, 'cuda_graph_bs': [1, 2, 4, 8, 12, 16, 24, 32, 40, 48, 56, 64], 'disable_cuda_graph': True, 'disable_cuda_graph_padding': False, 'enable_profile_cuda_graph': False, 'enable_cudagraph_gc': False, 'enable_layerwise_nvtx_marker': False, 'enable_nccl_nvls': False, 'enable_symm_mem': False, 'disable_flashinfer_cutlass_moe_fp4_allgather': False, 'enable_tokenizer_batch_encode': False, 'disable_tokenizer_batch_decode': False, 'disable_outlines_disk_cache': False, 'disable_custom_all_reduce': True, 'enable_mscclpp': False, 'enable_torch_symm_mem': False, 'disable_overlap_schedule': False, 'enable_mixed_chunk': False, 'enable_dp_attention': False, 'enable_dp_lm_head': False, 'enable_two_batch_overlap': False, 'enable_single_batch_overlap': False, 'tbo_token_distribution_threshold': 0.48, 'enable_torch_compile': False, 'enable_piecewise_cuda_graph': False, 'enable_torch_compile_debug_mode': False, 'torch_compile_max_bs': 32, 'piecewise_cuda_graph_max_tokens': 4096, 'piecewise_cuda_graph_tokens': [4, 8, 12, 16, 20, 24, 28, 32, 48, 64, 80, 96, 112, 128, 144, 160, 176, 192, 208, 224, 240, 256, 288, 320, 352, 384, 416, 448, 480, 512, 640, 768, 896, 1024, 1152, 1280, 1408, 1536, 1664, 1792, 1920, 2048, 2176, 2304, 2432, 2560, 2688, 2816, 2944, 3072, 3200, 3328, 3456, 3584, 3712, 3840, 3968, 4096], 'piecewise_cuda_graph_compiler': 'eager', 'torchao_config': '', 'enable_nan_detection': False, 'enable_p2p_check': False, 'triton_attention_reduce_in_fp32': False, 'triton_attention_num_kv_splits': 8, 'triton_attention_split_tile_size': None, 'num_continuous_decode_steps': 1, 'delete_ckpt_after_loading': False, 'enable_memory_saver': False, 'enable_weights_cpu_backup': False, 'enable_draft_weights_cpu_backup': False, 'allow_auto_truncate': False, 'enable_custom_logit_processor': False, 'flashinfer_mla_disable_ragged': False, 'disable_shared_experts_fusion': False, 'disable_chunked_prefix_cache': True, 'disable_fast_image_processor': False, 'keep_mm_feature_on_device': False, 'enable_return_hidden_states': False, 'enable_return_routed_experts': False, 'scheduler_recv_interval': 1, 'numa_node': None, 'enable_deterministic_inference': False, 'rl_on_policy_target': None, 'enable_attn_tp_input_scattered': False, 'enable_nsa_prefill_context_parallel': False, 'enable_fused_qk_norm_rope': False, 'enable_dynamic_batch_tokenizer': False, 'dynamic_batch_tokenizer_batch_size': 32, 'dynamic_batch_tokenizer_batch_timeout': 0.002, 'debug_tensor_dump_output_folder': './', 'debug_tensor_dump_layers': None, 'debug_tensor_dump_input_file': None, 'debug_tensor_dump_inject': False, 'disaggregation_mode': 'null', 'disaggregation_transfer_backend': 'mooncake', 'disaggregation_bootstrap_port': 8998, 'disaggregation_decode_tp': None, 'disaggregation_decode_dp': None, 'disaggregation_prefill_pp': 1, 'disaggregation_ib_device': None, 'disaggregation_decode_enable_offload_kvcache': False, 'num_reserved_decode_tokens': 512, 'disaggregation_decode_polling_interval': 1, 'encoder_only': False, 'language_only': False, 'encoder_transfer_backend': 'zmq_to_scheduler', 'encoder_urls': [], 'custom_weight_loader': [], 'weight_loader_disable_mmap': False, 'remote_instance_weight_loader_seed_instance_ip': None, 'remote_instance_weight_loader_seed_instance_service_port': None, 'remote_instance_weight_loader_send_weights_group_ports': None, 'remote_instance_weight_loader_backend': 'nccl', 'remote_instance_weight_loader_start_seed_via_transfer_engine': False, 'enable_pdmux': False, 'pdmux_config_path': None, 'sm_group_num': 8, 'mm_max_concurrent_calls': 32, 'mm_per_request_timeout': 10.0, 'enable_broadcast_mm_inputs_process': False, 'enable_prefix_mm_cache': False, 'mm_enable_dp_encoder': False, 'mm_process_config': {}, 'decrypted_config_file': None, 'decrypted_draft_config_file': None, 'forward_hooks': None, 'use_mla_backend': False, 'last_gen_throughput': 0.5018596586556519, 'memory_usage': {'weight': 2.46, 'kvcache': 45.7, 'token_capacity': 1497216, 'graph': 0}}], 'version': '0.5.6.post2'}, 'duration': 22.60802397504449, 'completed': 128, 'total_input_tokens': 481256, 'total_input_text_tokens': 481256, 'total_input_vision_tokens': 0, 'total_output_tokens': 128, 'total_output_tokens_retokenized': 127, 'request_throughput': 5.661706664027373, 'input_throughput': 21286.955486743416, 'output_throughput': 5.661706664027373, 'total_throughput': 21292.617193407445, 'mean_e2e_latency_ms': np.float64(2808.297436896737), 'median_e2e_latency_ms': np.float64(2510.6539335101843), 'std_e2e_latency_ms': np.float64(793.1095433281708), 'p99_e2e_latency_ms': np.float64(4556.360698912176), 'mean_ttft_ms': np.float64(2788.6635894769825), 'median_ttft_ms': np.float64(2455.6390999932773), 'std_ttft_ms': np.float64(830.4010082080309), 'p99_ttft_ms': np.float64(4556.355165026034), 'mean_tpot_ms': np.float64(0.0), 'median_tpot_ms': np.float64(0.0), 'std_tpot_ms': np.float64(0.0), 'p99_tpot_ms': np.float64(0.0), 'mean_itl_ms': np.float64(0.0), 'median_itl_ms': np.float64(0.0), 'std_itl_ms': np.float64(0.0), 'p95_itl_ms': np.float64(0.0), 'p99_itl_ms': np.float64(0.0), 'concurrency': np.float64(15.899756313049247), 'accept_length': None, 'max_output_tokens_per_s': 17.0, 'max_concurrent_requests': 32, 'input_lens': [3764, 3751, 3742, 3748, 3757, 3753, 3767, 3757, 3761, 3757, 3751, 3774, 3756, 3755, 3742, 3754, 3785, 3749, 3762, 3766, 3759, 3764, 3774, 3773, 3789, 3744, 3748, 3758, 3733, 3759, 3768, 3749, 3746, 3768, 3748, 3763, 3751, 3757, 3788, 3742, 3747, 3768, 3760, 3752, 3763, 3751, 3755, 3754, 3780, 3731, 3766, 3748, 3775, 3762, 3763, 3757, 3774, 3765, 3787, 3757, 3767, 3766, 3755, 3757, 3749, 3740, 3759, 3765, 3775, 3770, 3764, 3757, 3767, 3756, 3760, 3783, 3720, 3764, 3763, 3746, 3766, 3751, 3767, 3775, 3771, 3752, 3768, 3770, 3753, 3764, 3755, 3763, 3781, 3767, 3742, 3752, 3742, 3757, 3784, 3765, 3768, 3767, 3768, 3742, 3754, 3762, 3781, 3769, 3760, 3751, 3747, 3734, 3749, 3756, 3761, 3769, 3752, 3731, 3762, 3765, 3755, 3778, 3770, 3750, 3758, 3777, 3776, 3769], 'output_lens': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'ttfts': [2.1666875460068695, 2.312387409969233, 2.3128289140295237, 2.312300830031745, 2.31201149802655, 2.310504189983476, 2.3116103559732437, 2.3104055580333807, 2.31111971300561, 2.310695799998939, 2.309423113008961, 2.3102510279859416, 2.309176701004617, 2.3093911730102263, 2.3084663369809277, 2.308303044992499, 2.512673985969741, 2.5143066459568217, 2.5130424079834484, 2.513156309025362, 2.5127921670209616, 2.5136634719674475, 2.5124713749974035, 0.0, 2.5123215039493516, 2.511964611010626, 2.5118241909658536, 2.511664549994748, 2.51058031298453, 2.511193096986972, 2.5108171040192246, 2.510716243996285, 2.088105298986193, 2.2999950750381686, 2.299639313016087, 2.299156459979713, 2.2994196320069022, 2.2990690500009805, 2.298167454020586, 2.298059802968055, 2.298263245029375, 2.2978731030016206, 2.29642165300902, 2.2968046159949154, 2.296236033027526, 2.296851885970682, 2.5785777260316536, 2.5785289849736728, 4.5884219509898685, 4.556390105979517, 4.556260695040692, 4.5554560699965805, 4.554879356990568, 4.55464959598612, 4.55358577898005, 4.554750746989157, 4.553891971008852, 4.553168467013165, 4.55333615798736, 4.553012755000964, 4.552436853002291, 4.552238060976379, 4.448721293010749, 4.44848678103881, 1.8373926879721694, 1.942984698980581, 1.9423097560065798, 1.9417685010121204, 1.9412872690008953, 1.9414072299841791, 1.9400457410374656, 1.9402895020321012, 1.9404054139740765, 1.9401890119770542, 1.9396842989954166, 1.9389783450169489, 2.1388702479889616, 2.1383313249680214, 1.9688041859772056, 1.9684135340503417, 2.266998604987748, 2.400697887002025, 2.4005330950021744, 2.4003618540009484, 2.400115502998233, 2.4003043239936233, 2.3991560569847934, 2.3989566860254854, 2.398804465017747, 2.398755185015034, 2.398308541974984, 2.398211641004309, 2.356735948997084, 2.356139365991112, 2.355540173011832, 2.3556072029750794, 1.9903963770484552, 3.1827444219961762, 3.1824320800369605, 3.1817977650207467, 3.1818026049877517, 3.180309366027359, 3.180912301002536, 3.180484188022092, 3.1803906070417725, 3.1798283239477314, 3.179789953981526, 3.337850262003485, 3.1864660740247928, 3.1864881739602424, 3.1860264309798367, 3.1858158510294743, 4.330309404991567, 3.284412717970554, 3.283612583007198, 3.2830808910075575, 3.2829525300185196, 3.28274545900058, 3.28220155503368, 3.28209724399494, 3.281940573011525, 3.2805790960555896, 3.2814332399866544, 3.2213180550024845, 3.221196874976158, 3.220799923001323, 3.2204142099944875, 3.2203148690168746], 'itls': [[], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], [], []], 'generated_texts': ['.', '�', '�', 'ure', '�', 'ch', '�', '�', '.', '�', '�', '�', '\r\n', '�', '�', '�', 's', '�', '.', 'у', 'unic', 'д', 's', '', '�', '�', '�', 'z', '�', '�', '�', '�', '�', 'es', '.', '```', '�', '�', '�', '�', ' F', '�', '/', '�', '�', '�', 'it', '�', '�', '�', '�', 'er', '�', '_', '�', '�', '�', '�', '�', '�', 'z', '�', '�', ')', '�', '�', '�', 'side', '�', 'ing', 'change', '�', '�', 's', '�', '�', '�', '�', 'it', 're', '�', '�', '�', '_', '�', '�', 's', 'ur', '�', '�', '�', '�', 'ال', '�', '�', 'es', '�', 'ider', 'в', 'I', '�', '�', '�', '�', '�', 'h', '�', 'ng', '�', '�', '�', '�', ' B', '�', '�', ' F', 't', '�', '_', 'tern', '�', '.', ' B', '�', '�', '�', 'I', '�'], 'errors': ['', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']}
