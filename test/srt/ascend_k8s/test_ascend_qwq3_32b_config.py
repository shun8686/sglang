from test_ascend_single_mix_utils import NIC_NAME

QWQ_32B_MODEL_PATH = "/root/.cache/modelscope/hub/models/vllm-ascend/QWQ-32B-W8A8"

QWQ_32B_ENVS = {
    # "SGLANG_SET_CPU_AFFINITY": "1",
    "PYTORCH_NPU_ALLOC_CONF": "expandable_segments:True",
    "INF_NAN_MODE_FORCE_DISABLE": "1",
    "SGLANG_DISAGGREGATION_BOOTSTRAP_TIMEOUT": "600",
    "HCCL_BUFFSIZE": "2048",
    "HCCL_SOCKET_IFNAME": NIC_NAME,
    "GLOO_SOCKET_IFNAME": NIC_NAME,
    "HCCL_OP_EXPANSION_MODE": "AIV",
}

QWQ_32B_OTHER_ARGS = [
    "--trust-remote-code",
    "--nnodes",
    "1",
    "--node-rank",
    "0",
    "--attention-backend",
    "ascend",
    "--device",
    "npu",
    "--quantization",
    "modelslim",
    "--max-running-requests",
    "16",
    "--context-length",
    "8192",
    "--dtype",
    "bfloat16",
    "--chunked-prefill-size",
    "32768",
    "--max-prefill-tokens",
    "458880",
    "--disable-radix-cache",
    "--tp-size",
    "4",
    "--enable-dp-lm-head",
    "--mem-fraction-static",
    "0.68",
    "--cuda-graph-bs",
    2,
    4,
    6,
    8,
    10,
    12,
    16,
]
